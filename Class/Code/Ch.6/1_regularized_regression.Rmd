---
title: "An Introduction to Regularization"
author: "Jaime Davila"
date: "4/4/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
```

# Introduction: the Credit dataset

Today we will be using the `Credit` dataset from the `ISLR2` package. More information about this dataset can be found in
https://rdrr.io/cran/ISLR2/man/Credit.html. In particular we 
are interested in predicting the balance based on the other variables from the dataset

```{r echo=TRUE}
library(ISLR2)
data(Credit)
credit.tbl <- as_tibble(Credit)
credit.tbl
```

And as usual, we will be creating training and testing datasets using `tidymodels()`

```{r echo=TRUE}
library(tidymodels)
tidymodels_prefer()

set.seed(654321)
credit.split <- initial_split(credit.tbl, prop=0.8)
credit.train.tbl <- training(credit.split)
credit.test.tbl <- testing(credit.split)
```

# A first approach: A linear model

Let's start by creating a recipe called `credit.recipe`. In particular we would like:

* To predict `Balance` using all the other variables as explanatory variables (in R this can be written as `Balance ~.`).

* We would like to encode all categorical variables using indicator variables (using `step_dummy()`). Notice all categorical variables are binary, except for `Region` that has 3 different levels.

```{r echo=TRUE}
credit.recipe <- 
  recipe(formula=Balance ~ ., data=credit.train.tbl) %>%
  step_dummy(all_nominal_predictors())
```

Let's also remember that in linear regression we obtain coefficients $\beta_0$, $\beta_1, \dots, \beta_p$ that minimize

$$ RSS := \sum\limits_{i=1}^n \left(
y_i - \beta_0 - \sum \limits_{i=1}^p \beta_j x_{ij}
\right)^2$$


1. Set up the rest of the model and fit it using the training dataset. Create a table with all the coefficients of your linear model. Interpret the coefficients corresponding to:

* `Income`
* `Student_Yes`
* `Region_South`

What is the value of the $R^2$ on the testing dataset?

2. We can rate the importance of the different variables in a linear model by using the absolute value of their $t$-statistic (fourth term in your tibble generated by `tidy()`). Sort the variables according to the absolute value of the statistic. Compare your results with the graph generated by the `vip()` function available below:

```{r echo=TRUE}
library(vip)
extract_fit_parsnip(lm.fit) %>%
  vip()
```


# A first attempt at regularization: Ridge regression

Ridge regression works by changing the objective function that we want to minimize. Instead of just minimizing the RSS, we minimize the following function:


$$RSS + \lambda \sum\limits_{i=1}^p \beta_j^2$$
Notice that the $\beta_i$ are the coefficients of your linear model and $\lambda$ is a parameter to be defined. If $\lambda=0$ the problems becomes the same as linear regression and as $\lambda$ increases it has the effect of reducing the coefficients $\beta_i$ towards zero.

We can set up our ridge regression as follows:

```{r echo=TRUE}
ridge.model <- 
  linear_reg(mixture = 0, penalty=1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")
```

Notice that `mixture=0` selects a ridge regression and `mixture=1` selects a lasso regression (more on this later). Finally the `penalty` is the value that we give to $\lambda$ in our minimization function.

When we are dealing with a regularized model we need to make sure to normalize all of our explanatory variables using `step_normalize` as below:

```{r echo=TRUE}
ridge.recipe <- 
  recipe(formula = Balance ~ ., data = credit.train.tbl) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

And finally we can put it all together using:

```{r echo=TRUE}
ridge.wf <- workflow() %>% 
  add_recipe(ridge.recipe) %>% 
  add_model(ridge.model)

ridge.fit <- fit(ridge.wf, credit.train.tbl)
tidy(ridge.fit)
```

3. 
   a. Use the `vip` function to see the relative importance of each input variable and compare it with the results that you obtain from your linear model
   

   b. Notice that you can change the value of the penalty (or $\lambda$) as a parameter in the function `tidy()`. Experiment with several values of penalties (for example $\lambda=32,128,256$) and see what is the effect on the coefficients of your ridge model

4.  Compare the following plot with the left panel of figure 6.4 from ISLR. How do you interpret the graph? How do you interpret the line in black and to what coefficient does it correspond to?

```{r echo=TRUE}
ridge.fit %>%
  extract_fit_engine() %>%
  plot(xvar = "lambda")
```

# Calculating the optimal $\lambda$

In order to calculate the optimal $\lambda$ we will be using cross-validation on our training dataset. The following exercise will guide you in how to accomplish this:


5. Use 10  fold cross-validation to find the optimal $\lambda$ for this model by doing the following steps

a. Create a 10 fold cross validation tibble from `credit.train.tbl`

b. Create a ridge model specifying that the `penalty` will be **tuned**

c. Create a penalty grid that takes 20 values between -2 to 2

```{r}
penalty.grid <-
  grid_regular(penalty(range = c(-2, 2)), levels = 20)
```

d. Use `tune_grid()` to evaluate the model on the different parameters of the grid and plot the effect of the parameter on the fit of the model. 

e. Select the best penalty maximizing $R^2$  and calculate the $R^2$ of this model on your testing dataset. Compare the $R^2$ with the one you obtained using your linear model.

