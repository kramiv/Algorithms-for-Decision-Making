---
title: "LASSO Regression"
author: "Jaime Davila"
date: "4/6/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
library(tidyverse)
```

# Previously on ADM

Let's recall the steps to set-up our ridge regression model

## Dataset loading and training/testing split 

We can load our dataset by doing: 

```{r echo=TRUE}
library(ISLR2)
data(Credit)
credit.tbl <- as_tibble(Credit)
```

And create our training/testing datasets by:

```{r echo=TRUE}
library(tidymodels)
tidymodels_prefer()

set.seed(654321)
credit.split <- initial_split(credit.tbl, prop=0.8)
credit.train.tbl <- training(credit.split)
credit.test.tbl <- testing(credit.split)
```

## Setting up the ridge regression model

First we set-up our ridge model below. Notice that `mixture=0` and that set-up our `penalty` (or $\lambda$ ) to be tuned later on:

```{r echo=TRUE}
ridge.model <- 
  linear_reg(mixture = 0, penalty=tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")
```

Below we create our recipe and our workflow. Notice that we need to use `step_normalize()` to make sure all the variables are standarized.

```{r echo=TRUE}
ridge.recipe <- 
  recipe(formula = Balance ~ ., data = credit.train.tbl) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

ridge.wf <- workflow() %>% 
  add_recipe(ridge.recipe) %>% 
  add_model(ridge.model)
```

## Setting up the parameters for the optimization

Below we create our 10-fold cross-validation dataset using `vfold_cv()` and we create a logarithmic grid using `penalty()`

```{r echo=TRUE}
credit.fold <- vfold_cv(credit.train.tbl, v = 10)

penalty.grid <-
  grid_regular(penalty(range = c(0, 2)), levels = 20)
```

## Optimizing the penalty ($\lambda$)

The following code allows to optimize the penalty parameter

```{r echo=TRUE}
tune.res <- tune_grid(
  ridge.wf,
  resamples = credit.fold, 
  grid = penalty.grid
)
autoplot(tune.res)
```

And finally we can:

* Select the best parameter
* Explore the influence of lambda on the coefficients
* Calculate our $R^2$ using the testing dataset 
* Show the importance of the variables

```{r echo=TRUE}
(best.penalty <- select_best(tune.res, metric = "rsq"))
ridge.final.wf <- finalize_workflow(ridge.wf, best.penalty)
ridge.final.fit <- fit(ridge.final.wf, data = credit.train.tbl)

ridge.final.fit %>%
  extract_fit_engine() %>%
  plot(xvar = "lambda")

augment(ridge.final.fit, new_data = credit.test.tbl) %>%
  rsq(truth = Balance, estimate = .pred)

library(vip)
extract_fit_parsnip(ridge.final.fit) %>%
  vip()
```

# LASSO regression

When we were dealing with ridge regression we minimized the following function:


$$RSS(\beta_0,\dots,\beta_p) + \lambda \sum\limits_{i=1}^p \beta_j^2$$

The key thing to realize is that we are optimizing over the choice of our coefficients $\beta_0,\dots,\beta_p$ and that $\lambda$ is a fixed value which will be finding later on.

In LASSO we minimize a slightly different optimization function with profound consequences on our model. The function is:

$$RSS(\beta_0,\dots,\beta_p) + \lambda \sum\limits_{i=1}^p |\beta_j|$$

The following exercises will walk you through the setting up of the LASSO model and how the result differs from the ridge regression

1. Set up a LASSO regression model by using `tidymodels()` making sure you use the parameter `mixture=1`. For your $\lambda$ use a value of 1. Calculate the $R^2$ and the order of importance of variables

2. Compare the following plot to fig 6.6 from ISLR (left panel). How do you interpret this plot? What do you think the numbers on the upper side of the plot mean (Notice they go like 10,9,7,5,4,2,0)? How is this plot different from the corresponding plot from the ridge regression?

```{r echo=TRUE}
lasso.fit %>%
  extract_fit_engine() %>%
  plot(xvar = "lambda")
```

3. Experiment with your penalty grid and use cross-validation optimize the parameter $\lambda$. Calculate your $R^2$ on your testing dataset and determine the importance of the features. Compare this to your results using ridge regression.


# A word on the geometry of LASSO and ridge

Ridge and LASSO regression can be interpreted as minimizing $RSS(\beta_0,\dots, \beta_p)$ subject to the constraint (in this case note that $s$ is a fixed parameter)

* For ridge regression, $\sum \limits_{j=1}^p (\beta_j)^2 \le s$
* For LASSO regression, $\sum \limits_{j=1}^p |\beta_j| \le s$

One way to think about this is that we are allowed a budget of $s$ that we are spending across the sum of the $\beta_j^2$ for ridge and the sum of $|\beta_j|$ for LASSO. Notice that if $s$ tends to infinity we end up with just linear regression and that if $s$ is very small we force all of the coefficients to go to zero (Implying that $s$ and $\lambda$ from our first formulation are inversely related)

Furthermore notice that if $p=2$ the constraint for ridge consists of the unit disc with radius $\sqrt(s)$, while the constraint for LASSO is a diamond with diagonal $\frac{s}{2}$. Check figure 6.7 from ISLR from an illustration of how this observation implies that in LASSO we tend to choose values on the corners of the diamond, hence we are preferring solutions were the input variables are equal to zero.

# The College dataset

In the following set of exercises we will be exploring the `College` dataset from the `ISLR2` package. For more information on this dataset please consult `?College`. Let's start by loading our dataset and creating a training/testing dataset

```{r}
data(College)
college.tbl <- as_tibble(College) %>%
  select(-c(Accept, Enroll))

set.seed(123456)
college.split <- initial_split(college.tbl, prop=0.8)
college.train.tbl <- training(college.split)
college.test.tbl <- testing(college.split)
```

We are interested in predicting `Apps` based on the other variables.

4. Create a linear model to predict `Apps` based on the other variables. What is the $R^2$ on the testing dataset? Plot the input variable importance

5. Create a LASSO model to predict `Apps` based on the other variables. What is the $R^2$ on the testing dataset? Plot the input variable importance

6. Select the top-5 most important variables from LASSO and create a linear model based on only these features. How does the $R^2$ on the testing dataset compare with the full linear model?

7. Construct a KNN model using the top-5 most important variables from LASSO. Based on the $R^2$ on the testing dataset, is this a better model than 6?

8. What will be the topic for your Challenge 2 spotlight? Write a paragraph explaining the topic, dataset or question that you will be addressing.


