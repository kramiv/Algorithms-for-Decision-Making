---
title: "Introduction to Decision Trees"
author: "Jaime Davila"
date: "4/18/2021"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.show="hide", results=FALSE)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
library(rpart)
library(rpart.plot)
```

# Introduction

```{r include=FALSE}
scooby_raw <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-13/scoobydoo.csv")

scooby_raw %>%
  filter(monster_amount > 0) %>%
  count(monster_real)
```

```{r include=FALSE}
set.seed(123)
scooby.tbl <- scooby_raw %>%
  mutate(
    imdb = parse_number(imdb),
    year_aired = lubridate::year(date_aired)
  ) %>%
  filter(monster_amount > 0, !is.na(imdb)) %>%
  mutate(
    monster_real = case_when(
      monster_real == "FALSE" ~ "fake",
      TRUE ~ "real"
    ),
    monster_real = factor(monster_real)
  ) %>%
  select(year_aired, imdb, monster_real, title) 

write_csv(scooby.tbl, "~/Mscs 341 S22/Class/Data/scooby.csv"          )
```

On today's class we will be using a dataset collated from the popular animated series, Scooby Doo:

```{r echo=TRUE, results=TRUE}
(scooby.tbl <- read_csv("~/Mscs 341 S22/Class/Data/scooby.csv") %>%
  mutate(monster_real=factor(monster_real)))
table(scooby.tbl$monster_real)
```

In particular we are interested in predicting whether or not the monster in the episode is a real or fake based on the year that the episode was aired and how well liked it was on imdb. A preliminary plot shows the relationship across variables:


```{r echo=TRUE, fig.show="hide"}
scooby.tbl %>%
   ggplot(aes(imdb, year_aired))+
    geom_jitter(alpha = 0.7, width = 0.05, 
              height = 0.2, aes(color = monster_real))
```

And as usual we will set-up or training/testing dataset:

```{r echo=TRUE, results=TRUE}
set.seed(123)
scooby.split <- initial_split(scooby.tbl)
scooby.train.tbl <- training(scooby.split)
scooby.test.tbl <- testing(scooby.split)
```

# Recursive partitioning trees

Decisions trees introduce a completely new idea for making predictions. The fundamental  idea, as the name implies, is to use a **tree** as the means of making a decisions. The tree is built on a sequence of decisions based on the predictor variables. 

The easiest way to show a decision is to do an example using our dataset. A couple of things to notice from our code are:

* We use the library `rpart` 
* We will be making use of trees with 2 only levels (notice the parameter `tree_depth` below)

```{r echo=TRUE, results=TRUE}
scooby.model <-
  decision_tree(tree_depth=2) %>%
  set_mode("classification") %>%
  set_engine("rpart")

scooby.recipe <- recipe(monster_real ~ imdb+year_aired,
                 data=scooby.train.tbl)

scooby.wflow <- workflow() %>%
    add_recipe(scooby.recipe) %>%
    add_model(scooby.model) 

scooby.fit <- fit(scooby.wflow, scooby.train.tbl)
```

Finally notice that we can get a text output of our tree by using `scooby.fit`

```{r echo=TRUE, results=TRUE}
scooby.fit
```

A better way to visualize our decision tree is to use the library `rpart.plot`

```{r echo=TRUE, fig.show='asis'}
library(rpart.plot)
scooby.fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

1. 
    a. Talk for a couple of minutes to the people in your group about how to interpret all of the elements of the previous visualization.

    b. Look in page 336 of ISLR for definition of the Gini index and entropy. Why are those two quantities a measure of the "purity" of your partition? In what context are those two measures used?

Another way to visualize the data when you have only two predictors is to use the library `partree` as below

```{r echo=TRUE, fig.show='asis'}
library(parttree)
scooby.train.tbl %>%
  ggplot(aes(imdb, year_aired)) +
  geom_parttree(data = scooby.fit, 
                aes(fill = monster_real), alpha = 0.2) +
  geom_jitter(alpha = 0.7, width = 0.05, 
              height = 0.2, aes(color = monster_real))
```

2. Calculate the accuracy and the confusion matrix using your testing dataset

```{r}
augment(scooby.fit, new_data=scooby.test.tbl ) %>%
   accuracy(truth = monster_real, estimate = .pred_class)

augment(scooby.fit, new_data=scooby.test.tbl ) %>%
  conf_mat(truth = monster_real, estimate = .pred_class)
```

3. Calculate the accuracy of your model on your testing dataset for `tree_depth` values of 3, 5, 10 and visualize your models using `rpart.plot`. How do the different models compare to each other? Make sure to define and use a function that takes `tree_depth` as parameter

```{r}
test_tree <- function (depth) {
  scooby.model <-
  decision_tree(tree_depth=depth) %>%
  set_mode("classification") %>%
  set_engine("rpart")

  scooby.recipe <- recipe(monster_real ~ imdb+year_aired,
                 data=scooby.train.tbl)

  scooby.wflow <- workflow() %>%
    add_recipe(scooby.recipe) %>%
    add_model(scooby.model) 

  scooby.fit <- fit(scooby.wflow, scooby.train.tbl)

  scooby.fit %>%
  extract_fit_engine() %>%
  rpart.plot()

  augment(scooby.fit, new_data=scooby.test.tbl ) %>%
   accuracy(truth = monster_real, estimate = .pred_class)

}

test_tree(3)
test_tree(5)
test_tree(10)
```

4. The complexity parameter (`cp` or `cost_complexity`) is a key metric that penalizes the construction of large trees. Create a function `test_cp_tree` which takes as input the complexity parameter and visualizes the model and outputs its accuracy. Test the function for values of `cp`= 0.01, 0.1, 1. What is the effect of the smaller value of `cp` on your tree model?

```{r}
test_cp_tree <- function (cp) {
  scooby.model <-
  decision_tree(cost_complexity=cp) %>%
  set_mode("classification") %>%
  set_engine("rpart")

  scooby.recipe <- recipe(monster_real ~ imdb+year_aired,
                 data=scooby.train.tbl)

  scooby.wflow <- workflow() %>%
    add_recipe(scooby.recipe) %>%
    add_model(scooby.model) 

  scooby.fit <- fit(scooby.wflow, scooby.train.tbl)

  scooby.fit %>%
  extract_fit_engine() %>%
  rpart.plot()

  augment(scooby.fit, new_data=scooby.test.tbl ) %>%
   accuracy(truth = monster_real, estimate = .pred_class)
}

test_cp_tree(0.01)
test_cp_tree(0.1)
test_cp_tree(1)
```

# Optimizing our parameters

We are interested in optimizing our parameters using a cross-validation approach. As before the steps for doing so are:

* Make sure the parameters that you will be optimizing are tuneable.
* Create a cross validation dataset
* Create a grid for searching the parameters in your dataset
* Use `tune_grid` to optimize your function across the values of your grid

```{r echo=TRUE}
# Create the model with tuneable parameters
scooby.model <-
  decision_tree(tree_depth=tune(),
                cost_complexity=tune()) %>%
  set_mode("classification") %>%
  set_engine("rpart")

  scooby.recipe <- recipe(monster_real ~ imdb+year_aired,
                 data=scooby.train.tbl)

  scooby.wflow <- workflow() %>%
    add_recipe(scooby.recipe) %>%
    add_model(scooby.model) 
```


```{r echo=TRUE}
# Create the cross-validation dataset
set.seed(1234)
scooby.folds <- vfold_cv(scooby.train.tbl, v = 10)

```

```{r echo=TRUE}
#Set up the grid
scooby.grid <- 
  grid_regular(cost_complexity(), tree_depth(), levels = 4)

scooby.grid
```

```{r echo=TRUE, fig.show='asis'}
scooby.res <-
  tune_grid(
    scooby.wflow,
    resamples = scooby.folds,
    grid = scooby.grid,
    metrics = metric_set(accuracy, roc_auc, sensitivity, specificity))

```

5. Visualize `scooby.res` and discuss the results with your group members.

```{r}
autoplot(scooby.res)

```

6. Use `select_by_one_std_err` using accuracy as your metric and sorting in descending order the penalty parameter. Check the help of ?select_by_one_std_err and page 236 of ISLR to explain how the "one-standard-error" rule works. Use this parameter to finalize your workflow and fit it. What is your accuracy using your testing dataset?

```{r}
show_best(scooby.res, metric = "accuracy")
(best.penalty <- select_by_one_std_err(scooby.res, 
                                       metric = "accuracy", 
                                       -cost_complexity))
scooby.final.wf <- finalize_workflow(scooby.wflow,
                                     best.penalty)

scooby.final.fit <- fit(scooby.final.wf,  scooby.train.tbl)

scooby.final.rs <- last_fit(scooby.final.wf, 
                         scooby.split)

collect_metrics(scooby.final.rs)
```

Let's visualize our model on our training dataset using `parttree`

```{r echo=TRUE, fig.show='asis'}
scooby.train.tbl %>%
  ggplot(aes(imdb, year_aired)) +
  geom_parttree(data = scooby.final.fit, aes(fill = monster_real), alpha = 0.2) +
  geom_jitter(alpha = 0.7, width = 0.05, height = 0.2, aes(color = monster_real))
```

And let's look at how the final model looks as a tree

```{r}
scooby.final.fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint=FALSE)
```



# Acknowledgments

This worksheet draw heavily on the following blog post from Julia Silge: https://juliasilge.com/blog/scooby-doo/


