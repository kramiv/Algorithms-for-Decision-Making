---
title: "Logistic regression in tidymodels"
author: "Jaime Davila"
date: "3/7/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
```

# Introduction

Let's start by loading an old friend of ours, the `Default` dataset (remember Homework 3?)

```{r}
library(ISLR2)
data(Default)
default.tbl <- tibble(Default)
```

We are interested in predicting whether a person would go on *default* (that is, would not pay back a loan) given the following information:

* `balance` (How much does the person owe?)
* `income` (How much does the person earn?)
* `student` (Is the person a student?)

0. How many observations does this dataset have? How many defaults and non-defaults? How many defaults by student status?

```{r}
dim(default.tbl)
table(default.tbl$default)
table(default.tbl$default, default.tbl$student)
```

1. Create a boxplot of `balance` across people who default or not. What do you observe? What do you observe when you facet the boxplot by `student`?

2. How about the effect of `income` on defaulting? Does it change according to `student` status?

3. Load `tidymodels` and create a training dataset using $8000$ observations and a testing dataset using $2000$ observations. Make sure to call your training dataset `default.train.tbl` and your testing dataset `default.test.tbl`

```{r echo=TRUE}
set.seed(12345)
```

# Modeling probabilities with linear models

Let's start by trying to predict the default using a linear model whose input variable is `balance`, that is 

$$ y = \alpha_0 + \alpha_1 \times balance$$
In homework 3 we learned that using linear models in this setting creates a number of issues for classification, among them the fact that we are not guaranteed that the prediction will correspond to a probability (a number between `0` and `1`)

One way to overcome this is to use the log odds on our response variable. The log odds $y$ of an event with probability $p$ is defined as

$$\tilde y :=\log \left(\frac{p}{1-p}\right)$$

Hence, if $\tilde y$ represents the log odds, we can invert this expression to get the probability.

$$p=\frac{e^{\tilde y}}{1+e^{\tilde y}}=\frac{1}{1+e^{-\tilde y}}$$

# Logistic regression using `tidymodels`

Fortunately for us, logistic models are readily available in R. We can create such a model on the training dataset using the following code:

```{r echo=TRUE}
logit.model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

default.recipe <- 
  recipe(default ~ balance, data=default.train.tbl)

logit.wflow <- workflow() %>%
  add_recipe(default.recipe) %>%
  add_model(logit.model) 

logit.fit <- fit(logit.wflow, default.train.tbl)
```

Finally we can predict the probability of defaulting on the testing dataset or simply predict whether someone would go on default or not

```{r echo=TRUE}
predict(logit.fit, default.test.tbl, type="prob")
predict(logit.fit, default.test.tbl)
```

4. Plot the predicted probability of defaulting (using the logit model) as a function of `balance`.

5. How many observations are predicted to default in your testing dataset? How many of your predictions are wrong?

6. `yardstick` allows you to evaluate the performance of your classification model in many different ways. Consult https://www.tmwr.org/performance.html#binary-classification-metrics and do the following:

a. Calculate the confusion matrix of your model. Is your model making more errors on people that go on default or not?

b. Define `accuracy` and calculate it for your model.

c. Define `specificity`, `sensitivity`, `ROC`, and `AUC`. Plot the ROC curve and calculate the AUC of your model.

7. (*Optional*) Calculate the AUC and plot the ROC for the logistic regression model that takes into account `balance`, `income` and `student`.

8. (*Optional*) Plot the decision boundary for the logistic regression model with inputs of balance, income and student.



