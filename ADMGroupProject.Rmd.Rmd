---
title: "ADMFirstChallenge"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---


```{r setup, include = FALSE}
#>>=====<<Packages>>========================================<<
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
library(dslabs)
library(ISLR2)
library(tidymodels)
```

```{r}
#Importing Dataset
mnist <- read_mnist("~/Mscs 341 S22/Class/Data")
str(mnist)
```

# **Dataset Creation**

**• Your dataset should have in total 1000 randomly selected digits (feel free to use a set.seed command so that your results are reproducible).**

First, calculate values for an individual image:
```{r echo = T, results = 'hide'}
set.seed(12345) #For Reproducible Values
```

```{r}
index <- sample(1:60000, 60000) #vector of randomly selected indexes to sample from mnist

tester <- 
  as_tibble(index) %>%
  mutate(image = mnist$train$images[value, ]) %>%
  mutate(label = mnist$train$labels[value]) %>%
  dplyr::filter(label == 0 | label == 7)

tester$image[10,] #this pulls out a single image. This will be plotted to confirm if it's correct
tester$label[10] #Obtaining label to see if it matches with image. This is labelled 0.
```

Plot the image to ensure data is extracted correctly. It should be a 0.
```{r}
#Function that makes the image
plotImage <- function(dat,size=28){
  imag <- matrix(dat,nrow=size)[,28:1]
  image(imag,col=grey.colors(256), xlab = "", ylab="") 
}

#Plotting the image of 0.
plotImage(tester$image[10,])
```
Looks like a 0. Data above was extracted correctly.

**• Your training dataset should have 800 observations and your testing should have 200 observations.**

Dividing the dataset into testing and training:
```{r}
#Getting 1000 rows
tester <- tester %>%
  slice(1:1000)

#Setting Seed
set.seed(12345)

#Splitting into training and testing with respective ratio of 4:.1
trainer <- slice_sample(tester, n = 800)
tester <- setdiff(tester, trainer)
```

• Use the mnist dataset from dslabs to create an end-to-end classifier that distinguishes between 7 and 0.

# **Feature Definition**

**• You are allowed to use only 2 features. Notice that you need to calculate those features directly from dataset. Make sure to describe what those features represent and why you chose them. Are those features capturing any intuition that you have about distinguishing those two digits?**

To determine the best way to calculate `x_1` and `x_2`, the quadrants will be calculated individually (q1, q2, q3, and q4) so they can be used to calculate different possible combinations of `x_1` and `x_2.` We will test `x_1` and `x_2` based on which appears to work best when plotted on a scatter plot.

## Features:

**Feature 1:** We test `x_1 = q1+q3-q2-q4` as a measure of symmetry. 0 should be much more symmetrical than 7, so values of `x_1` should be smaller for 0. 

**Feature 2:** `x_2` is the sum of darkness in pixels of the top left quadrant `q2`. We expect this to be smaller for 7 than for 0 because the top-left area of a 7 is not very large.

First, the four quadrants will be calculated. Choose a single vector of 0: (will also do this process for 7 as a comparison: tester$label[1])
```{r echo = T, results = 'hide'}
zeroMatrix <- matrix(trainer$image[4, ],nrow=28)[,28:1]
sum(zeroMatrix[1:14, 15:28])  #q1
sum(zeroMatrix[1:14, 1:14])   #q2
sum(zeroMatrix[15:28, 1:14])  #q3
sum(zeroMatrix[15:28, 15:28]) #q4

#Observing the values of the quadrants+
abs(sum(zeroMatrix[1:14, 15:28]) + sum(zeroMatrix[15:28, 1:14]) - sum(zeroMatrix[1:14, 1:14]) - sum(zeroMatrix[15:28, 15:28]))
```
`q1 = 6588;    q2 = 8456;    q3 = 7982;    q4 = 8562`

Repeating this process above for 7:
```{r echo = T, results = 'hide'}
sevenMatrix <- matrix(trainer$image[1, ],nrow=28)[,28:1]
sum(sevenMatrix[1:14, 15:28])  #q1
sum(sevenMatrix[1:14, 1:14])   #q2
sum(sevenMatrix[15:28, 1:14])  #q3
sum(sevenMatrix[15:28, 15:28]) #q4

abs(sum(sevenMatrix[1:14, 15:28]) + sum(sevenMatrix[15:28, 1:14]) - sum(sevenMatrix[1:14, 1:14]) - sum(sevenMatrix[15:28, 15:28]))
```
`q1 = 4821;    q2 = 1757;    q3 = 7921;    q4 = 4487`

The approach above `(q1+q3-q2-q4)` gives 2448 for the zero and 6498 for the 7 which is about what we would expect - ideally the value returned for the zero image would be closer to zero. Similarly, the difference in the values of `q1` and `q2` is stark and apparent. 

Next, calculate these values for all images and put it in a table:

### Making the training dataset

```{r echo = T, results = 'hide'}
#note that directly mutating these values into trainer or tester does not work. need to use a for-loop to create a vector of values that can then be mutated into trainer or tester.

trainVectorQ1 <- vector()#making sure the variables are clear
trainVectorQ2 <- vector()
trainVectorQ3 <- vector()
trainVectorQ4 <- vector()

for (i in 1:800) {#note: this value should be 1:200 for tester
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q1 = sum(sumMatrix[1:14, 15:28])#q1
  print(q1)
  trainVectorQ1 <- c(trainVectorQ1, q1)
  trainVectorQ1
}
for (i in 1:800) {
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q2 = sum(sumMatrix[1:14, 1:14])#q2
  trainVectorQ2 <- c(trainVectorQ2, q2)
  trainVectorQ2
}
for (i in 1:800) {
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q3 = sum(sumMatrix[15:28, 1:14])#q3
  trainVectorQ3 <- c(trainVectorQ3, q3)
  trainVectorQ3
}
for (i in 1:800) {
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q4 = sum(sumMatrix[15:28, 15:28])#q4
  trainVectorQ4 <- c(trainVectorQ4, q4)
  trainVectorQ4
}
```

```{r}
trainer <- trainer %>%
  select(label) %>%
  mutate(label=as.factor(label)) %>%
  mutate(row = row_number()) %>%
  mutate(q1 = trainVectorQ1) %>%
  mutate(q2 = trainVectorQ2) %>%
  mutate(q3 = trainVectorQ3) %>%
  mutate(q4 = trainVectorQ4) %>%
  mutate(x_1 = abs(q1+q3-q2-q4)) %>%
  mutate(x_2 = q2)

trainer
```

Plot the classifers:
```{r}
trainer %>%
  ggplot(aes(x = x_1, y = x_2, color = label)) +
    geom_point()
```
`x_1` and `x_2` appear to successfully separate 0 and 7. There is not much overlap between the coordinate points of 0 and 7.


### Repeat the process above to create the tester tibble and plot:

```{r echo = F, results = 'hide'}
testVectorQ1 <- vector()#making sure the variables are clear
testVectorQ2 <- vector()
testVectorQ3 <- vector()
testVectorQ4 <- vector()
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q1 = sum(sumMatrix[1:14, 15:28])#q1
  print(q1)
  testVectorQ1 <- c(testVectorQ1, q1)
  testVectorQ1
}
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q2 = sum(sumMatrix[1:14, 1:14])#q2
  testVectorQ2 <- c(testVectorQ2, q2)
  testVectorQ2
}
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q3 = sum(sumMatrix[15:28, 1:14])#q3
  testVectorQ3 <- c(testVectorQ3, q3)
  testVectorQ3
}
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q4 = sum(sumMatrix[15:28, 15:28])#q4
  testVectorQ4 <- c(testVectorQ4, q4)
  testVectorQ4
}

tester <- tester %>%
  select(label) %>%
  mutate(label=as.factor(label)) %>%
  mutate(row = row_number()) %>%
  mutate(q1 = testVectorQ1) %>%
  mutate(q2 = testVectorQ2) %>%
  mutate(q3 = testVectorQ3) %>%
  mutate(q4 = testVectorQ4) %>%
  mutate(x_1 = abs(q1+q3-q2-q4)) %>%
  mutate(x_2 = q2)

tester %>%
  ggplot(aes(x = x_1, y = x_2, color = label)) +
    geom_point()+
    labs(title = "")
```

```{r}
head(tester)
```

This looks similar to the plot for trainer which is what is expected. Since there are fewer points, the difference between the points for 0 and 7 are not as defined. Next, these tables will be used to create and test models.

# **Model creation, optimization, and selection**

**• Create at least two different models for this classification and make sure to optimize the parameters those models have.**

**• Calculate the misclassification rates for both models and select the model with the lowest error rate.**

## Model #1

The first model we will be using is the logistic regression model from parsnip in tidymodels. 

```{r echo = T, results = 'hide'}
logit.model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

default.recipe <- 
  recipe(label ~ x_1+x_2, data=trainer)

logit.wflow <- workflow() %>%
  add_recipe(default.recipe) %>%
  add_model(logit.model) 

logit.fit <- fit(logit.wflow, trainer)
logit.fit
```

Now, this model can be used on the tester dataset to classify whether images are 0 or 7 (in addition to the probability of being a value being 0 or 7).

```{r}
predict(logit.fit, tester, type = "prob") #Gives probability of 0 and 7
predict(logit.fit, tester) #Gives the classsification
```

Calculate the misclassification rate:
```{r}
misclassification.tbl <- augment(logit.fit, tester)
mean(misclassification.tbl$label != misclassification.tbl$.pred_class)
```
The misclassification rate is 11.5%. 

## Model #2

Now, we are going to use K nearest neighbours (knn) from parsnip in tidymodels.
`nearest_neighbor()` uses k = 5 as default, let us attempt to optimize it

```{r}
library(kknn)

#Making the model
knn.model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

#Making the workflow
knn.wflow <- workflow() %>%
  add_recipe(default.recipe) %>%
  add_model(knn.model) 

```

Optimizing K:
```{r}
#Making 10-fold cross-validation dataset
digits.folds <- vfold_cv(trainer, v = 10) 
training(digits.folds$splits[2][[1]])
testing(digits.folds$splits[2][[1]])

#Making grid of neighbours across values of K
neighbors.tbl <-  tibble(neighbors = seq(1,51, by = 1))
neighbors.grid.tbl <- grid_regular(neighbors(range = c(1, 51)), 
                                   levels = 51)

#Tuning the results accordingly
tune.results <- tune_grid(object = knn.wflow, 
                          resamples = digits.folds, 
                          grid = neighbors.tbl)

#Having a look at the values of K
autoplot(tune.results)

#Show the best Value of K
show_best(tune.results, metric = "accuracy")
best.neighbor <- select_best(tune.results, metric = "accuracy")

#Applying the optimal value of K (14)
knn.final.wflow <- finalize_workflow(knn.wflow, best.neighbor)
knn.fit <- fit(knn.final.wflow, trainer)
```

Finally, getting missclassification rate
```{r}
predict(knn.fit, tester, type = "prob")
predict(knn.fit, tester)
```

```{r}
misclassification.tbl <- augment(knn.fit, tester)
mean(misclassification.tbl$label != misclassification.tbl$.pred_class)
```

**Result of the models:** Logistic Regression gave a misclassification rate of 11.5% and Knn gave 12.5%. Hence Logistic is marginally better. However logistc works with only 2 variables. Hence another model will be tested.

## Model #3
```{r}
library(tidymodels)
library(discrim)
tidymodels_prefer()

lda.model <- discrim_linear() %>%
  set_engine("MASS") %>% #MASS is the library, or a type of implementation
  set_mode("classification")

lda.wflow <- workflow() %>%
  add_recipe(default.recipe) %>%
  add_model(lda.model) 

lda.fit <- fit(lda.wflow, trainer)
lda.fit
```

```{r}
predict(lda.fit, tester, type="prob")
predict(lda.fit, tester)
```

```{r}
misclassification.tbl <- augment(lda.fit, tester)
mean(misclassification.tbl$label != misclassification.tbl$.pred_class)
```

Misclassification = 12.5%

# Visualization

**• Plot the probabilities across a grid and the decision boundary for your selected model**

As shown above, both Knn and LDA show an equally good misclassification rate For the purpose of this project, we are going to use Knn. The probability of an image being 7 will be plotted with a decision boundary.

First, we found max and min values for `x_1` and `x_2` to create a grid for plotting with Bayes' boundary. Using max and min, the largest `x_1` is `20518`, and the largest `x_2` is 18572.
We create grids using seq up to 21000.

```{r}
grid.vec.x_1 <- seq(from = 0, to = 21000, by = 500)
grid.vec.x_1
grid.vec.x_2 <- seq(from = 0, to = 21000, by = 500)
grid.tbl <- expand_grid(x_1 = grid.vec.x_1, x_2 = grid.vec.x_2)
```

```{r}
pred <- predict(knn.fit, grid.tbl, type="prob")#probability
pred %>%
  mutate(x_1 = grid.tbl$x_1) %>%
  mutate(x_2 = grid.tbl$x_2) %>%
  ggplot(aes(x_1, x_2, z=.pred_7,fill = .pred_7)) +
    geom_raster() +
    stat_contour(breaks=c(0.5), color="pink")+
    scale_fill_viridis_b()+
    labs(title = "Decision Boundary Based on Knn Model",
         fill = "Probability of 7")
```

# **Changing things up: adding the number 5**

**• Create a new dataset that includes your two chosen digits and the digit 5. Create training and testing datasets that include 5 and your two given digits.**

**• Calculate the same 2 features for this new testing and training dataset.**

We copy and paste the code from near the beginning of this document and then plot it to confirm that everything looks as it should. This time, 5 is also included in the testing and training tables.

Training Dataset:
```{r echo = F, results = 'hide'}
set.seed(12345)
index <- sample(1:60000, 60000)
tester <- as_tibble(index) %>%
  mutate(image = mnist$train$images[value, ]) %>%
  mutate(label = mnist$train$labels[value]) %>%
  dplyr::filter(label == 0 | label == 7 | label == 5)
tester <- tester %>%
  slice(1:1000)
set.seed(12345)
trainer <- slice_sample(tester, n = 800)
tester <- setdiff(tester, trainer)

trainVectorQ1 <- vector()#making sure the variables are clear
trainVectorQ2 <- vector()
trainVectorQ3 <- vector()
trainVectorQ4 <- vector()
for (i in 1:800) {#note: this value should be 1:200 for tester
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q1 = sum(sumMatrix[1:14, 15:28])#q1
  print(q1)
  trainVectorQ1 <- c(trainVectorQ1, q1)
  trainVectorQ1
}
for (i in 1:800) {
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q2 = sum(sumMatrix[1:14, 1:14])#q2
  trainVectorQ2 <- c(trainVectorQ2, q2)
  trainVectorQ2
}
for (i in 1:800) {
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q3 = sum(sumMatrix[15:28, 1:14])#q3
  trainVectorQ3 <- c(trainVectorQ3, q3)
  trainVectorQ3
}
for (i in 1:800) {
  sumMatrix <- matrix(trainer$image[i, ], nrow = 28)[,28:1]
  q4 = sum(sumMatrix[15:28, 15:28])#q4
  trainVectorQ4 <- c(trainVectorQ4, q4)
  trainVectorQ4
}

trainer <- trainer %>%
  select(label) %>%
  mutate(label=as.factor(label)) %>%
  mutate(row = row_number()) %>%
  mutate(q1 = trainVectorQ1) %>%
  mutate(q2 = trainVectorQ2) %>%
  mutate(q3 = trainVectorQ3) %>%
  mutate(q4 = trainVectorQ4) %>%
  mutate(x_1 = abs(q1+q3-q2-q4)) %>%
  mutate(x_2 = q2)
```

Having a look at the split between the parameters and the numbers
```{r}
trainer %>%
  ggplot(aes(x = x_1, y = x_2, color = label)) +
    geom_point()+
    labs(title="Scatterplot of Features for Training Data With 0, 5, & 7")
```
5 Appears to be right in the middle of the split of 0 and 7.

Testing Dataset:
```{r echo = F, results = 'hide'}
testVectorQ1 <- vector()#making sure the variables are clear
testVectorQ2 <- vector()
testVectorQ3 <- vector()
testVectorQ4 <- vector()
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q1 = sum(sumMatrix[1:14, 15:28])#q1
  print(q1)
  testVectorQ1 <- c(testVectorQ1, q1)
  testVectorQ1
}
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q2 = sum(sumMatrix[1:14, 1:14])#q2
  testVectorQ2 <- c(testVectorQ2, q2)
  testVectorQ2
}
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q3 = sum(sumMatrix[15:28, 1:14])#q3
  testVectorQ3 <- c(testVectorQ3, q3)
  testVectorQ3
}
for (i in 1:200) {
  sumMatrix <- matrix(tester$image[i, ], nrow = 28)[,28:1]
  q4 = sum(sumMatrix[15:28, 15:28])#q4
  testVectorQ4 <- c(testVectorQ4, q4)
  testVectorQ4
}

tester <- 
  tester %>%
  select(label) %>%
  mutate(label=as.factor(label)) %>%
  mutate(row = row_number()) %>%
  mutate(q1 = testVectorQ1) %>%
  mutate(q2 = testVectorQ2) %>%
  mutate(q3 = testVectorQ3) %>%
  mutate(q4 = testVectorQ4) %>%
  mutate(x_1 = abs(q1+q3-q2-q4)) %>%
  mutate(x_2 = q2)
```

```{r}

#Making the model
knn.model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

#Making the new Recipe
default.recipe <- recipe(label ~ x_1+x_2, data=trainer)

#Making the workflow
knn.wflow <- workflow() %>%
  add_recipe(default.recipe) %>%
  add_model(knn.model) 

```

Optimizing K:
```{r}
#Making 10-fold cross-validation dataset
digits.folds <- vfold_cv(trainer, v = 10) 
training(digits.folds$splits[2][[1]])
testing(digits.folds$splits[2][[1]])

#Making grid of neighbours across values of K
neighbors.tbl <-  tibble(neighbors = seq(1,51, by = 1))
neighbors.grid.tbl <- grid_regular(neighbors(range = c(1, 51)), 
                                   levels = 51)

#Tuning the results accordingly
tune.results <- tune_grid(object = knn.wflow, 
                          resamples = digits.folds, 
                          grid = neighbors.tbl)

#Having a look at the values of K
autoplot(tune.results)

#Show the best Value of K
show_best(tune.results, metric = "accuracy")
best.neighbor <- select_best(tune.results, metric = "accuracy")

#Applying the optimal value of K (14)
knn.final.wflow <- finalize_workflow(knn.wflow, best.neighbor)
knn.fit <- fit(knn.final.wflow, trainer)
```

Calculating Misclassification rate:
```{r}
augment(knn.fit, tester) %>%
  accuracy(truth = label, estimate = .pred_class)
```
We get a misclassification of 33.5%

Confusion Matrix:
```{r}
augment(knn.fit, tester) %>%
  conf_mat(truth = label, estimate = .pred_class)
```
5 appears to be problematic as a lot of them seem to be confused for 7s and a comparable amount seem to be confused with 0s. However, the model seems to work especially well for 0 and 7 isn't that bad either. 7 is very rarely confused for a 0 but is often confused for a 5. 

Results: 

Accuracy of 0: `56/(56+9+3) = 0.8235 -> 82.3%` Very good!

Accuracy of 5: `29/(13+29+17) = 0.4915 -> 49.1%` Mediocre

Accuracy of 7: `48/(5+20+48) = 0.6575 -> 65.7%` Fair

As we might expect, since our features are based on 0 and 7, the 5s are misclassified the most. Out of the misclassifications of 5s, 26/37 = 70% were classified as 7s. The other 30% were misclassified as 0s. The reason why our model thinks 5s are 7s more often is because, like the average 7, the average 5 is not going to be perfectly symmetric, and we can expect most 5s to have some of their image in the upper left corner. If we ran our model with just 0s and 5s, we would expect to have a misclassification rate similar to our model for classifying 0s and 7s.

Creating the Grid:
```{r}
create_grid <- function(delta) {
  expand_grid(x_1=seq(0,21000, by=delta), x_2 = seq(0,21000, by=delta))
}
grid.tbl <- create_grid(200)
grid.tbl <- grid.tbl %>%
  mutate(x_1 = as.integer(x_1)) %>%
  mutate(x_2 = as.integer(x_2))

augment.tbl <- predict(knn.fit, grid.tbl) %>%
  mutate(row = row_number())
augment2.tbl <- predict(knn.fit, grid.tbl, type = "prob") %>%
  mutate(row = row_number())

augment.tbl <- full_join(augment.tbl, augment2.tbl, by = "row") %>%
  mutate(x_1 = grid.tbl$x_1) %>%
  mutate(x_2 = grid.tbl$x_2)
augment.tbl
```  

Plot Grid With Boundary
```{r}
augment.tbl %>%
    ggplot() +
      geom_raster(aes(x_1, x_2, fill = .pred_class)) +
      geom_point(data=tester, aes(x=x_1, y=x_2, color=label, shape=label))+
      scale_color_manual(values=c("blue","red","orange"))+
      labs(title="Decision Boundary of LDA for 0, 5, & 7",
           fill = "Predicted Class",
           shape = "Label")
```
