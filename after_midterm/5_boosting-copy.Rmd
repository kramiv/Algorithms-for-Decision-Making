---
title: "Boosting models "
author: "Jaime Davila/Matt Richey"
date: "5/4/2022"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.show="hide", results=FALSE)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(tidymodels)
library(dslabs)
tidymodels_prefer()
```


suppose:
one input variable, one response variable.
suppose data is split along two y-lines. a bunch of points. 
create a linear fit y = x which is the same as the average value. 
then, plot the residuals in a different graph. 

now, create a 2nd model fit which approximates the residuals. it will be a tree. The tree is some function that jumps around (f0, f1, ...). f_1 = tree depth fit of residuals. 

f_1 = lambda(f0 + f1). 

next: get f_2 where I take residuals of this plot. with lines f0, f1, ...

f_2 = lambda(f0 + f1). 

fitting model to the residuals over and over again. 

remember: random forest strategy. little random trees that I fit to my data. maximal data trees. 

in this: fit many simple models. Each model is not independent of the other models. can plot the learning rate vs. accuracy.

****Boosting and random forest often have similar performance, but it depends on the problem. 
****is this for regression or classification?

# Introduction

The idea behind boosting is that we employ "weak learning" over and over again in order to get a suitable model.  With boosting, we build a large number of  minimal (underfitted) models. From these underfitted models, aka **weak learners**,  we build a final  model by adding the underfitted models. 

In general, any form of base model can work, but boosting most often uses trees. Hence, in what follows, the weak learner will involve small trees, usually with  depth =1 (stumps) or depth=2. 

Suppose we have data $X$, a response variable $y$ and a learning rate $\lambda >0$ . We will  build a sequence of weak learners (small trees), $\hat f_b$, $b=1...B$ as follows.

  * Initialize the **residuals** $r=y$ and  the **initial model** $\hat f(x)=0$. 

  * For $b=1,2,\dots,B$, 
    * Fit a weak learner $\hat f_b(x)$ to the $X$ and $r_{i-1}$ (the current response). 
    * Update $\hat f(x)$ via
    $$\hat f(x) \leftarrow \hat f(x)+\lambda \hat f_b(x)$$
    * Update the residuals
    $$r \leftarrow r-\lambda \hat f_b(X)$$

  * When done, the boosted model is:
  $$\hat f(x)=\lambda(\hat f_1(x)+\cdots+\hat f_B(x)).$$


# Using boosting on the election data: regression.

Let's see how this works on an old example, the 2008 election dataset. As usual, let's load our dataset and divide into training/testing datasets

```{r echo=TRUE}
data("polls_2008")
polls.2008.tbl <- tibble(polls_2008)
polls.2008.tbl

set.seed(12345)
poll.split <- initial_split(polls.2008.tbl)
poll.train.tbl <- training(poll.split)
poll.test.tbl <- testing(poll.split)
```

And let's start by loading the `xgboost` library and create a skeleton of our model describition using `use_xgboost` from the `usemodels` package

```{r echo=TRUE, results=TRUE}
library(xgboost)
library(usemodels)
use_xgboost(margin~day, poll.train.tbl, tune=FALSE)
```

We will copy the generated code and pack in it a function. Notice our function will have parameters for:

 * the number of trees (`num_trees` or $B$ in our previous description) 
 * the learning rate (`learn.rate` or $\lambda$ from our intro)
 * the tree depth (`tree.depth`) for the depth of the weak learners (trees) that we will be using

All of these parameters we pass on to the function `boost_tree()`

```{r echo=TRUE}
create_boost <- function(poll.train.tbl, num.trees, learn.rate, tree.depth) {
  xgboost_recipe <- 
    recipe(formula = margin ~ day, data = poll.train.tbl) %>% 
    step_zv(all_predictors()) 

  xgboost_spec <- 
    boost_tree(trees=num.trees, learn_rate=learn.rate, tree_depth=tree.depth) %>% 
    set_mode("regression") %>% 
    set_engine("xgboost") 

  xgboost_workflow <- 
    workflow() %>% 
    add_recipe(xgboost_recipe) %>% 
    add_model(xgboost_spec) 
  
  fit(xgboost_workflow, poll.train.tbl)
}
```

And as before we will have functions that will allow us to calculate our error and plot our model:

```{r echo=TRUE}
calc_rmse <- function(model, test.tbl) {
  augment(model, test.tbl) %>%
    rmse(margin, .pred) %>% 
    pull(.estimate)
}

plot_model <- function(model, test.tbl) {
  augment(model, test.tbl) %>%
    ggplot()+
    geom_point(aes(day,margin))+
    geom_step(aes(day,.pred), col="red")
}
```

The following set of exercises will illustrate how the parameters for the boosting model work:

1. Let's start by fixing our `tree_depth=1` (so we will be using trees of just one level) and our `learn.rate=1` (a relatively aggressive learning rate)

    a. Create a boosting model using only one tree and plot the resulting model. Explain why the resulting model is a constant line

```{r}
num.trees = 1
learn.rate = 1
tree.depth = 1

model1a <- create_boost(poll.train.tbl, num.trees, learn.rate, tree.depth)
calc_rmse(model1a, poll.test.tbl)
plot_model(model1a, poll.test.tbl)
```
rmse = 0.02345104

it is a single line because num trees = 1 and tree depth = 1. Most importantly, num trees = 1.
A smaller learning rate helps straighten the line as well.

prof:
for (i in 2:5) {
boost.model <- create_model(...)
rmse <- round(calc_rmse(...))
print(plot_model) +
  ggtitle(str_c("Trees=", i, " rmse=", rmse))
}

    b. Now try values of number of trees 2,3,4,5 and plot the resulting model and calculate the rmse. Explain why increasing the number of trees results in a model with better rmse

```{r}
learn.rate = 1
tree.depth = 1

for (val in 1:5) {

num.trees = val

model1a <- create_boost(poll.train.tbl, num.trees, learn.rate, tree.depth)
print(calc_rmse(model1a, poll.test.tbl))
print(plot_model(model1a, poll.test.tbl))
  
}

```
[1] 0.02345104
[1] 0.01886486
[1] 0.01625665
[1] 0.01779611
[1] 0.01581384

more lines on the line appear = better fit for each chunk of data. 
  
    c. Now try values of number of trees 10,20,50,100 and plot the resulting model and calculate the rmse. Are we overfitting the data when we use too many trees?
    
```{r}
learn.rate = 1
tree.depth = 1

x <- c(10, 20, 50, 100)

for (val in x) {

num.trees = val

model1a <- create_boost(poll.train.tbl, num.trees, learn.rate, tree.depth)
print(calc_rmse(model1a, poll.test.tbl))
print(plot_model(model1a, poll.test.tbl))
}
```
[1] 0.01590197
[1] 0.01597511
[1] 0.01737759
[1] 0.01735754

rmse increases after 10 trees. Too many trees = overfitting.
    
    
2. The second important parameter of a boosting model is our $\lambda$ or learning rate. Notice the learning measures how much contribution each individual model will be making to the boosting model. 

    a. Let's start by fixing our number of trees to 100 and our tree depth to 1. Try learning rates of 1, 0.5, 0.2, 0.1, 0.05 and plot the resulting model and calculate the rmse. What is the effect of the smaller learning rates on the plot?
    
```{r}
tree.depth = 1
num.trees = 100

x <- c(0.05, 0.1, 0.2, 0.5, 1.0)

for (val in x) {
  
learn.rate = val

model1a <- create_boost(poll.train.tbl, num.trees, learn.rate, tree.depth)
print(calc_rmse(model1a, poll.test.tbl))
print(plot_model(model1a, poll.test.tbl))
}
```
[1] 0.01588569
[1] 0.01526684
[1] 0.01539111
[1] 0.01605805
[1] 0.01735754

smaller learning rate = smoother line. the line's lines are smaller. 
smaller learning rate = smaller rmse, but too small also bad. best one was 0.1 here. 

    b. Finally let's try using 1000 trees and try learning rates from of  0.5, 0.1, 0.05, 0.01, 0.005. Describe your results from the plot and the rmse.
    
```{r}
tree.depth = 1
num.trees = 1000

x <- c(0.005, 0.01, 0.05, 0.1, 0.5)

for (val in x) {
  
learn.rate = val

model1a <- create_boost(poll.train.tbl, num.trees, learn.rate, tree.depth)
print(calc_rmse(model1a, poll.test.tbl))
print(plot_model(model1a, poll.test.tbl))
}
```
[1] 0.01596947
[1] 0.01527843
[1] 0.01552335
[1] 0.01592984
[1] 0.01732603

best one is 2nd one. smallest rmse for learning rate = 0.01. graph looks relatively smooth but not super boxy. 
for some reason 


overall: 

num trees and learning rate = not too big, not too small. 

    
It is clear from the previous exercises that optimizing the parameters for number of trees and for learning rate is very important when building a boosting model. We will start by setting up a grid with 5 levels for  these 2 parameters:


```{r echo=TRUE, results=FALSE}
(poll.grid <- grid_regular(trees(range=c(100,500)), learn_rate(range=c(-3,-1)), levels = 5))
```

****
why is learning rate negative range=c(-3,-1)
usually learning rate is on the log scale. 


And we will create a 10-fold cross validation dataset

```{r echo=TRUE}
poll.folds <- vfold_cv(poll.train.tbl, v = 10)
```

Finally we will use `use_xgboost` to create our template for our cross validation

```{r echo=TRUE, results=TRUE}
use_xgboost(margin~day, poll.train.tbl)
```

Notice that when copying the code

* We make sure to only set `trees`, and `learn_rate` as tuneable parameters in `xgboost_spec`
* We use `poll.folds` and `poll.grid` in our `tune_grid` function

```{r echo=TRUE}
xgboost_recipe <- 
  recipe(formula = margin ~ day, data = poll.train.tbl) %>% 
  step_zv(all_predictors()) 

xgboost_spec <- 
  boost_tree(trees = tune(), learn_rate = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost") 

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(xgboost_recipe) %>% 
  add_model(xgboost_spec) 

set.seed(24725)
xgboost_tune <-
  tune_grid(xgboost_workflow, 
            resamples = poll.folds, 
            grid = poll.grid)
```

The following plot illustrates how if you make your learning rate small enough you need to increase your number of trees to get a comparable `rmse`. However it seems for rates of `0.1` and `0.03` we reach saturation by the time we are using 100 trees.

```{r echo=TRUE, fig.show='asis'}
autoplot(xgboost_tune, metric="rmse", select_best=TRUE)
```


****
prof: boosting has a lot of parameters that you can tune. Note: cross-validation aobve only tunes learn rate and num trees. 

******
the prof uses some use_xboost() function that is not in this worksheet.


Finally let's select the optimal parameters and finalize our workflow

```{r echo=TRUE, results=TRUE}
(best.param <- select_best(xgboost_tune, "rmse"))
(poll.boost.wf <- finalize_workflow(xgboost_workflow, best.param))
poll.boost.model <- fit(poll.boost.wf, poll.train.tbl)
```

Let's plot our model on our complete dataset and calculate our `rmse` using our testing dataset

```{r echo=TRUE, results=TRUE, fig.show='asis'}
plot_model(poll.boost.model, polls.2008.tbl)
calc_rmse(poll.boost.model, poll.test.tbl)
```

[1] 0.01749392

  trees = 500
  learn_rate = 0.01

prof: says learning rate is most important? missed it. 



# Back to square one: The MNIST dataset: classification.

Before we go let's run a boosting model for MNIST dataset. As usual, let's load the dataset and subset to only 1000 images.

```{r echo=TRUE}
mnist <- read_mnist("~/Mscs 341 S22/Class/Data")
set.seed(2022)
index <- sample(nrow(mnist$train$images), 100)#was 1000
digit.train.tbl <- as_tibble (mnist$train$images[index,]) %>%
  mutate(digit = factor(mnist$train$labels[index]))

index <- sample(nrow(mnist$test$images), 100)#was 1000
digit.test.tbl <- as_tibble (mnist$test$images[index,]) %>%
  mutate(digit = factor(mnist$test$labels[index]))
```

And let's keep a couple of handy functions for plotting images.

```{r echo=TRUE, fig.show='asis'}
plotImage <- function(dat,size=28){
  imag <- matrix(dat,nrow=size)[,28:1]
  image(imag,col=grey.colors(256), xlab = "", ylab="") 
}

plot_row <- function(tbl) {
  ntbl <- tbl %>%
    select(V1:V784)
  plotImage(as.matrix(ntbl))
}

create_image_vip <- function(model.fit) {
  # Creates the importance image
  imp.tbl <- model.fit %>%
    extract_fit_engine() %>%
    vip::vi() %>%
    mutate(col=as.double(str_remove(Variable,"V")))
  mat <- rep(0, 28*28)
  mat[imp.tbl$col] <- imp.tbl$Importance
  mat
}
```

Also since we will be optimizing parameters using cross-validation, let's create a 5 fold cross-validation dataset and grid with 10 levels for the learning rate:

```{r echo=TRUE, results=TRUE}
set.seed(2022)
digit.folds <- vfold_cv(digit.train.tbl, v = 5)
(digit.grid <- grid_regular(learn_rate(range=c(-2,0)), levels = 10))
```

And let's use `use_xgboost` to create a template for our code

```{r echo=TRUE, results=TRUE}
use_xgboost(digit~., digit.train.tbl)
```

For the sake of time, we will be optimizing only `learn_rate` 

*****
tree_depth is automatic?

```{r echo=TRUE, results=TRUE}
xgboost_recipe <- 
  recipe(formula = digit ~ ., data = digit.train.tbl) %>% 
  step_zv(all_predictors()) 

xgboost_spec <- 
  boost_tree(learn_rate = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("xgboost") 

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(xgboost_recipe) %>% 
  add_model(xgboost_spec) 

set.seed(34854)
xgboost_tune <-
  tune_grid(xgboost_workflow, 
            resamples = digit.folds,
            grid=digit.grid)
```

And let's plot our optimization 

```{r echo=TRUE, fig.show='asis'}
autoplot(xgboost_tune, metric="accuracy", select_best=TRUE)
```

Finally let's select the optimal parameters, finalize our workflow, and calculate our accuracy and confusion matrix:

```{r echo=TRUE, results=TRUE}
(best.param <- select_best(xgboost_tune, "accuracy"))
(digit.boost.wf <- finalize_workflow(xgboost_workflow, best.param))
digit.boost.model <- fit(digit.boost.wf, digit.train.tbl)

augment(digit.boost.model, digit.test.tbl) %>%
  accuracy(truth=digit, estimate= .pred_class)

augment(digit.boost.model, digit.test.tbl) %>%
  conf_mat(truth=digit, estimate= .pred_class)
```

accuracy = 0.54

(.85 in class with 1000 digits)

*****
"So it seems boosting in the case of digit recognition is not as good as a random forest, but it is not too far off the mark."
Note: for random forest, accuracy was about 90%.

Finally, let's look at our variable importance:

```{r fig.show="asis", echo=TRUE}
create_image_vip(digit.boost.model) %>%
  plotImage()
```

It seems that not as many pixels are important as in a random forest, but still it takes information from lots of places to make the decision.

prof: 
when creating model, also need to consider training dataset. It is 1000 numbers here. 
The sample size also affects accuracy. 1000 vs. 1000 training dataset size. 
prof got 0.949 accuracy. random forest = 91% (with what sample size though?)


********
try rerunning the model without year. 








