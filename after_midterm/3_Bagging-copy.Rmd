---
title: "Bootstrap aggregation (bagging)"
author: "Jaime Davila"
date: "4/24/2021"
output:
  pdf_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

cost-complexity = ratio of improvement of mse between two levels has to be bigger than cost complexity. 

cp = 1: it has to improve 100%. very short tree. 
cp = 0: overfitting and fit the whole training dataset. 

min n is minimum number of elements for a partition to end up having. 

trees with cp = 0 are called "maximum trees". 

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.show="hide", results=FALSE)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(tidymodels)
library(dslabs)
tidymodels_prefer()
```

# Maximal classification trees

Let's start by considering the presidential polls dataset for the 2008 election (Obama vs McCain) and creating our testing/training dataset

```{r echo=TRUE}
library(dslabs)
data("polls_2008")
polls.2008.tbl <- tibble(polls_2008)
polls.2008.tbl

set.seed(12345)
poll.split <- initial_split(polls.2008.tbl)
poll.train.tbl <- training(poll.split)
poll.test.tbl <- testing(poll.split)
```

After this we will be creating a couple of auxiliary functions:

* `create_rtree`: fits a regression tree to `train.tbl` with a predefined complexity parameter `cp`.
* `calc_rmse`: calculates the rmse for `model` on `test.tbl`.
* `plot_model`: depicts the trends of the model using the testing dataset

```{r echo=TRUE}
create_rtree <- function(train.tbl, cp) {
  # Set up the model, recipe and workflow
  poll.model <-
  decision_tree(cost_complexity=cp) %>%
  set_mode("regression") %>%
  set_engine("rpart")
  poll.recipe <- recipe(margin ~ day, data=train.tbl)
  poll.wflow <- workflow() %>%
    add_recipe(poll.recipe) %>%
    add_model(poll.model) 
  # Fit the worfklow using the training dataset  
  fit(poll.wflow, train.tbl)
}

calc_rmse <- function(model, test.tbl) {
  augment(model, test.tbl) %>%
    rmse(margin, .pred) %>% 
    pull(.estimate)
}

plot_model <- function(model, test.tbl) {
  augment(model, test.tbl) %>%
    ggplot()+
    geom_point(aes(day,margin))+
    geom_step(aes(day,.pred), col="red")
}
```

And we will be testing this functions using **maximal classification trees**, which are trees for which the complexity parameter is 0. Notice that for this type of tree, usually the other default parameters of `rpart` (e.g. `tree_depth` or `min_n`) are the ones responsible for the tree not having a node for each observation.

```{r echo=TRUE, results=TRUE, fig.show='asis'}
rtree.model <- create_rtree(poll.train.tbl, 0) 
plot_model(rtree.model, poll.test.tbl)
calc_rmse(rtree.model, poll.test.tbl)
```

#key ideas for today: creating multiple regression trees. 
take training dataset and divide it into 3 pieces. (initial split)
for each of these, I fit a maximal regression tree. 
then take the average of the 3 regression trees. (how?)
each regression tree has a prediction. so combine them by averaging them. that will be the value of my new "bagging" model.  

1. Using `poll.train.tbl`, create 3 different training datasets.  Fit maximal regression trees to each of them. Using `poll.test.tbl` plot the models and calculate the rmse for each of them. Are the results very different depending on the training dataset used?

maximal regression tree means cp = 0.

*****
note how prof splits the tibbles

```{r echo=TRUE}
set.seed(12345)
poll.split.1 <- initial_split(poll.train.tbl)
train.tbl.23 <- training(poll.split.1)
train.tbl.1 <- testing(poll.split.1)

poll.split.2 <- initial_split(train.tbl.23)
train.tbl.2 <- training(poll.split.2)
train.tbl.3 <- testing(poll.split.2)
```
```{r}
dim(poll.train.tbl)
dim(train.tbl.1)
dim(train.tbl.2)
dim(train.tbl.3)
```
```{r}
#train.tbl.1:
model.1 <- create_rtree(train.tbl.1, 0)
calc_rmse(model.1, poll.test.tbl)
plot_model(model.1, poll.test.tbl)
```
[1] 0.02084598
```{r}
#train.tbl.2:
model.2 <- create_rtree(train.tbl.2, 0)
calc_rmse(model.2, poll.test.tbl)
plot_model(model.2, poll.test.tbl)
```
[1] 0.01734644
```{r}
#train.tbl.3:
model.3 <- create_rtree(train.tbl.3, 0)
calc_rmse(model.3, poll.test.tbl)
plot_model(model.3, poll.test.tbl)
```
[1] 0.02524915

rmse seems to hover about 0.02 for each model. It does fluctuate a good deal. 

2. Apply the previous 3 models to `poll.test.tbl` and create a new column `.pred` with the average of the 3 models. Plot the average and calculate the rmse. Are the results an improvement over the model at the beginning of this section?

```{r}
tbl1 <- augment(model.1, poll.test.tbl) %>%
  rename(.pred.1 = .pred)

tbl2 <- augment(model.2, poll.test.tbl) %>%
  rename(.pred.2 = .pred)

tbl3 <- augment(model.3, poll.test.tbl) %>%
  rename(.pred.3 = .pred)
tbl3

test.tbl <- full_join(tbl3, full_join(tbl1, tbl2, by = c("day", "margin")), by = c("day", "margin"))

test.tbl <- test.tbl %>%
  mutate(test.pred = (.pred.3 + .pred.1 + .pred.2)/3) %>%
  select(day, margin, test.pred)

test.tbl
```

***
plot and calculate mse:
```{r}
ggplot(test.tbl) +
  geom_point(aes(day,margin))+
    geom_step(aes(day,test.pred), col="red")

rmse(test.tbl, margin, test.pred)
```
rmse is an improvement over each of the single ones



bagging: when you change your training dataset, the maximum tree will be different. 
and if you take the average, the average is usually a better estimate. 


# Bagging models from scratch

Bootstrap aggregation or bagging is based on the following two key ideas:

* Maximal trees are very sensitive to their training dataset. Slightly different training datasets can result in very different trees.
 
* If we have $n$ *independent* variables $X_1, \dots, X_n$ with a standard deviation $\sigma$, the standard deviation of their average is $\frac {\sigma} {\sqrt n}$. This implies that we can control the error of a collection of maximal trees by considering their average (this principle is sometimes called "the wisdom of the crowd")

In bagging we average the results of multiple maximal trees that are trained on slightly different bootstrapped training datasets. Let's start by creating a function `create_bag_rtree` that creates a bootstrap of our training dataset `train.tbl`, fits a maximal tree and outputs the results of that tree evaluated on our testing dataset `test.tbl`

this time get like 20 trees. 
bootstrap = sampling with replacement. bootstrapping = sample with replacement set with exact number of elements. 

```{r echo=TRUE, fig.show="asis"}
create_bag_rtree <- function(id, train.tbl, test.tbl) {
  # Set up the bootstrap
  bootstrap.split <- bootstraps(train.tbl, times=1)#  take first bootstrap as training datset
  bootstrap.train.tbl <- analysis(bootstrap.split$splits[[1]])
  
  # Set up the model, recipe and workflow
  poll.model <-
  decision_tree(cost_complexity=0) %>%
  set_mode("regression") %>%
  set_engine("rpart")
  poll.recipe <- recipe(margin ~ day, data=bootstrap.train.tbl)
  poll.wflow <- workflow() %>%
    add_recipe(poll.recipe) %>%
    add_model(poll.model) 
  # Fit the worfklow using the training dataset  
  poll.fit <- fit(poll.wflow, bootstrap.train.tbl)
  augment(poll.fit, test.tbl) %>%
    mutate(id=id)
}
```

***
how is final_fit different from fit?


We will be doing this process 20 times by making use of the `map_dfr` function and we will plot each of our bootstrapped models. Notice how every model is slightly different than the others.

this creates a single tree
```{r echo=TRUE, fig.show="asis"}
set.seed(12345)
bag.tbl <- map_dfr(1:20, create_bag_rtree, poll.train.tbl, poll.test.tbl)

ggplot(bag.tbl)+
    geom_point(aes(day,margin))+
    geom_step(aes(day,.pred), col="red")+
    facet_wrap(vars(id))
```

Finally we will average and plot the results across each tree and calculate our rmse, which is an improvement over our previous results


notice id in bag.tbl represents a different bootstrap I think. 

average all the bootstraps for a 'single model'. Notice that this averaged model is just averaging a bunch of models and getting averaged prediction table. 

```{r echo=TRUE, results=TRUE, fig.show="asis"}
bag.summary.tbl <- bag.tbl %>%
  group_by(day) %>%
  summarize(.pred = mean(.pred),
            margin = mean(margin)) 

ggplot(bag.summary.tbl)+
  geom_point(aes(day,margin))+
    geom_step(aes(day,.pred), col="red")

rmse(bag.summary.tbl, margin, .pred )
```


  .metric .estimator .estimate
  <chr>   <chr>          <dbl>
1 rmse    standard      0.0158

# Bagging models using `tidymodels()`

 We will be using the implementation of bagging from the `ranger` package. As before we will be creating a function `create_bag` that takes a training dataset and the number of trees that we are combining (`ntrees`).

```{r echo=TRUE}
library(ranger)
create_bag <- function(train.tbl, ntrees) {
  # Set up the model, recipe and workflow
  poll.bag.model <-
  rand_forest(trees=ntrees) %>%
  set_mode("regression") %>%
  set_engine("ranger")

  poll.recipe <- recipe(margin ~ day, data=train.tbl)
  
  poll.wflow <- workflow() %>%
    add_recipe(poll.recipe) %>%
    add_model(poll.bag.model) 
  # Fit the worfklow using the training dataset  
  fit(poll.wflow, train.tbl)
}
```

First, let's look at the effect of adding trees one by one. Notice that as the number of trees becomes larger, our predicted value becomes less rugged.

```{r echo=TRUE, results=TRUE, fig.show='asis'}
set.seed(12345)
for (i in seq(1,5, by=1)) {
  bag.model <- create_bag(poll.train.tbl, i)
  print(calc_rmse(bag.model, poll.test.tbl))
  print(plot_model(bag.model, polls.2008.tbl))
}
```
[1] 0.02217587
[1] 0.01834043
[1] 0.01600297
[1] 0.01769001
[1] 0.01822232

looks like middle one is best with input of 3 for create_bag(


In bagging, usually the more the merrier, and around 100s of trees we get diminishing results. Let's settle for 100 trees and see the resulting model

```{r echo=TRUE, results=TRUE, fig.show='asis'}
set.seed(12345)
bag.model <- create_bag(poll.train.tbl,100)
calc_rmse(bag.model, poll.test.tbl)
plot_model(bag.model, polls.2008.tbl)
```
[1] 0.01699069

note: model still has many little crevices. Each individual tree has a relatively small number of nodes in each tree. min_n moderates this - keeps partition to at least n values. by default, n is usually like 10.

what is optimal minimum n?
--> use cross validation on minimum n. 

3. Using 10-fold cross validation and the following grid to identify the optimal value for `min_n`, fixing the number of trees to 100. What's the RMSE on the testing dataset? Plot your resulting model and compare it to the model where `min_n` was not optimized at the beginning of the section.
```{r echo=TRUE}
set.seed(12345)
poll.folds <- vfold_cv(poll.train.tbl, v = 10)
poll.grid <- grid_regular(min_n(), levels = 25)
```


```{r}
poll.tune.model <- rand_forest(min_n = tune(), trees= 100) %>%
  set_mode("regression") %>%
  set_engine("ranger")

poll.recipe <- recipe(margin ~ day, data=poll.train.tbl)

poll.wflow <- workflow() %>%
    add_recipe(poll.recipe) %>%
    add_model(poll.tune.model) 

poll.res <-#this bit here with tune_grid is what cross-validation is. tune, show best. these are the extra steps. (uses the grid and folds created)
  tune_grid(
    poll.wflow,
    resamples = poll.folds,
    grid = poll.grid)

autoplot(poll.res)
show_best(poll.res, metric = "rmse")
```

```{r}
(best.penalty <- select_by_one_std_err(poll.res, 
                                       metric = "rmse", 
                                       -min_n))

poll.final.wf <- finalize_workflow(poll.wflow, best.penalty)
poll.final.fit <- fit(poll.final.wf,  poll.train.tbl)
poll.final.rs <- last_fit(poll.final.wf, poll.split)
collect_metrics(poll.final.rs)
```
 min_n .metric .estimator   mean     n std_err .config                .best .bound
  <int> <chr>   <chr>       <dbl> <int>   <dbl> <fct>                  <dbl>  <dbl>
1    40 rmse    standard   0.0223    10 0.00177 Preprocessor1_Model25 0.0215 0.0231

  .metric .estimator .estimate .config             
  <chr>   <chr>          <dbl> <fct>               
1 rmse    standard      0.0154 Preprocessor1_Model1
2 rsq     standard      0.588  Preprocessor1_Model1

```{r}
augment(poll.final.fit, poll.train.tbl) %>%
    ggplot()+
    geom_point(aes(day,margin))+
    geom_step(aes(day,.pred), col="red")

poll.fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint=FALSE)
```

rmse = 0.0154
***
note: we have been looking at rmse, not r^2




bagging vs. bootstrapping: 

bootstrapping: creating multiple testing and training datasets. Create a maximum regression tree for each bootstrap training + testing set. 

bagging model will evaluate the average
*****
how do we find the ideal number of trees/bootstrap tables?

How does cross validation come in? We can maximize the ideal value for min_n. 

what about cost_complexity and tree_depth?

note: I average a bunch of trees with various min_n values. How does cross validation come in?
bagging is a function of my min_n. So, I can compare different parameters of n and select optimum value. 




















