---
title: "2 - Prediction"
author: "J. Williams"
date: "23 feb 2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
```

# Classification and choice of K

On a first instance we will be exploring our Minneapolis police incidents dataset in a more systematic way and construct visualizations that explore the effect of the value of $K$ in our KNN model.

Let's start by loading the dataset

```{r}
mn.police.tbl <- read_csv("~/Mscs 341 S22/Class/Data/police_incidents.mn.csv") 
```

And remember the steps that we took last time, namely:

1. Divide our dataset into training and testing datasets.
2. Construct a model based on the training dataset.
3. Evaluate the fit of the model by calculating the MSE on the testing dataset

These steps can be done as follows:

```{r}
# Divide dataset into testing and training
train.mn.police.tbl <- mn.police.tbl %>%
  filter(year==2016)
test.mn.police.tbl <- mn.police.tbl %>%
  filter(year==2017)

# Build the KNN model
kNear=7
knn.model <- knnreg(tot~week, data=train.mn.police.tbl,k=kNear)

# Evaluate the MSE of the KNN model in the testing dataset
test.pred <- predict(knn.model, test.mn.police.tbl)
(mse.test <- mean ((test.mn.police.tbl$tot-test.pred)^2))
```

We are interested in calculating systematically the MSE as we iterate over the parameter $k$ by doing the following steps: 

1. Create a function `calc_MSE(kNear, train.tbl, test.tbl)` that trains a KNN model with parameter `kNear` on `train.tbl` and then applies the model on `test.tbl` and calculates the MSE. Test your function with k=7 and k=35 using our testing and training datasets.
```{r}
calc_MSE <- function(kNear, train.tbl, test.tbl) {
  knn.model <- knnreg(tot~week, data=train.tbl, k=kNear)
  
  test.pred <- predict(knn.model, test.tbl)
  mean((test.tbl$tot - test.pred)^2)
}

calc_MSE(7, train.mn.police.tbl, test.mn.police.tbl)
calc_MSE(35, train.mn.police.tbl, test.mn.police.tbl)

```


2. We would like to create a vector with the MSE values for our testing dataset. Notice it only makes sense to look at values of k in increments of 7 (why?). Use a for loop in R(Look at the syntax of for loops in https://rafalab.github.io/dsbook/programming-basics.html#for-loops) to create the mse for $k=7,14,21,...,364$
```{r}
MSE.test <- vector(length = 52)
MSE.train <- vector(length = 52)
for (i in 1:52) {
  MSE.test[i] <- calc_MSE(i*7, train.mn.police.tbl, test.mn.police.tbl)
  MSE.train[i] <- calc_MSE(i*7, train.mn.police.tbl, train.mn.police.tbl)
}

MSE.test
MSE.train

```

3. Generate a graph depicting the MSE as a function of `k` for both testing and training datasets. What is the optimal value for `k` based on the testing dataset? Can you find this value in a systematic way? (*Hint*: Check the documentation for function `slice_min`). Compare this graph against figures 2.9 and 2.10 from your book. What would be the equivalent of the parameter `flexibility` in your KNN model?
```{r}
MSE.tbl <- tibble(k = 7*(1:52),
                  train = MSE.train,
                  test = MSE.test) %>%
  pivot_longer(2:3, names_to = "Dataset", values_to = "MSE")

ggplot(MSE.tbl, aes(x = k, y = MSE, color = Dataset)) +
  geom_line()

#optimal k = 145
MSE.tbl %>%
  filter(Dataset == "test") %>%
  slice_min(MSE, with_ties="false")

```

# Improving your model

One way to improve the performance of a model is to use the information provided by other variables. In the following exercises we will explore this in more detail:

4. Does the distribution of number of incidents across the days of the week? Generate a boxplot to explore this question and check if this behavior is consistent across the years
```{r}
days = unique(mn.police.tbl$wday)

mn.police.tbl %>%
  mutate(year = as.factor(year),
         wday = factor(wday, levels=days)) %>%
ggplot(aes(x = wday, y = tot, fill=as.factor(year))) +
  geom_boxplot()

# incidents cycle toward a peak on Mondays
```

5. It seems police incidents are higher on Monday as opposed to other days. Subset you dataset to only Mondays and construct a KNN model using `week` as input variable and train it using the data from 2016. Plot a graph with the MSE for all different choices of $k$ and select a $k$ that minimizes the MSE on the testing. Is the MSE smaller using this model than our original model?
```{r}
police.monday.tbl <- mn.police.tbl %>%
  filter(wday == "Mon", year==2016)

monday.test.tbl <- mn.police.tbl %>%
  filter(wday == "Mon", year==2017)

police.monday.tbl #training set

# create
MSE.monday <- vector(length = 52)
for (i in 1:52) {
  MSE.monday[i] <- calc_MSE(i, police.monday.tbl, monday.test.tbl)
}
monday.tbl <- tibble (k = 1:52,
                      mse = MSE.monday)

monday.tbl

# plot
ggplot(monday.tbl, aes(x = k, y = mse)) +
  geom_line()

best.k <- monday.tbl %>%
  slice_min(order_by = mse, with_ties = FALSE)

best.k # best k is 31, mse of 133
```

# Studying the COVID pandemic

For the next set of exercises we will be using the US covid dataset procured from [CovidActNow](covidactnow.org). We'll start by loading the dataset. Notice that I also loaded the library `lubridate` which allows for convenient use of dates.

```{r}
library(lubridate)
covid.tbl <- read_csv("~/Mscs 341 S22/Class/Data/covid.csv")
```

6. Subset your dataset from August 2020 to August 2021 and plot the median number of cases and the median number of deaths (per 100,000). *Hint*: You can filter dates using `filter` and you will need to create a reference date with `as.Date` 
```{r}
covid.tbl %>% 
  filter(date >= as.Date("2020-08-01"), date <= as.Date("2021-08-31")) %>%
  group_by(date) %>%
  mutate(med.cases = median(cases.100k),
         med.deaths = median(deaths.100k)) %>% # summarize(med.cases = median(cases.100k), med.deaths = median(deaths.100k)) %>%
  pivot_longer(6:7, names_to="type", values_to="values") %>%
    ggplot(aes(date,values)) +
      geom_line() +
    facet_grid(type ~.)+
  scale_y_continuous(trans = 'log10')

```

7. We would like to create a model that would be able to predict the number of deaths based on the number of cases. To do that let's create training and testing datasets from neighboring states, let's say WI and MN. Plot the number of cases against the number of deaths for your training dataset and include a linear trend in your plot
```{r}
covid.train.tbl <- covid.tbl %>%
  filter(state == "WI") %>%
  select(cases.100k, deaths.100k)

covid.test.tbl <- covid.tbl %>%
  filter(state == "MN") %>%
  select(cases.100k, deaths.100k)

ggplot(covid.train.tbl, aes(x = cases.100k, y = deaths.100k)) +
  geom_point() +
  geom_smooth(method = "lm")

```


8. Create a linear model using `lm()` using the data from WI (training) and evaluate how well it does in MN (testing).
```{r}
lm.model <-  lm(deaths.100k ~ cases.100k, data = covid.train.tbl)

death.pred <- predict(lm.model, covid.test.tbl)
mse.death.pred <- mean ((covid.test.tbl$deaths.100k-death.pred)^2)


mse.death.pred # MSE of 0.03550234
```

9. To improve our model, let's make use of the fact that the number of covid cases is a good predictor of the number of deaths a couple of weeks afterwards. Create a function `calc_MSE_lag(time.lag, train.tbl, test.tbl)` that calculates the MSE on the testing dataset by using a linear model where the deaths lag the number of cases by `time.lag` *Hint* Make use of the functions `lead/lag` from the tidyverse.
```{r}
calc_MSE_lag <- function(time.lag, train.tbl, test.tbl) {
  train.tbl <- train.tbl %>%
    mutate(lag.cases = lag(train.tbl$cases.100k, time.lag)) %>%
    filter(!is.na(lag.cases))
  
  test.tbl <- test.tbl %>%
    mutate(lag.cases = lag(test.tbl$cases.100k, time.lag)) %>%
    filter(!is.na(lag.cases))
  
  model = lm(deaths.100k ~ lag.cases, data = train.tbl)
  pred.test <- predict(model, test.tbl)
  
  mean((test.tbl$deaths.100k - pred.test)^2)
}

calc_MSE_lag(7, covid.train.tbl, covid.test.tbl)
calc_MSE_lag(14, covid.train.tbl, covid.test.tbl)
calc_MSE_lag(21, covid.train.tbl, covid.test.tbl)
```


10.  Plot lag versus MSE on the testing dataset and find the optimal parameter of lag. How do you interpret this optimal lag? Using this value of lag, plot the lagged number of cases versus the deaths and the linear trend line for the testing dataset.
```{r}
test.mse <- vector(length = 31)
for (i in 1:31) {
  test.mse[i] <- calc_MSE_lag(i, covid.train.tbl, covid.test.tbl)
}
test.mse

lags.tbl <- tibble (MSE = test.mse) %>%
  mutate(lags = 1:31)
lags.tbl

lags.tbl %>%
  ggplot(aes(x = lags, y = MSE)) +
  geom_line()

best.lag <- lags.tbl %>%
  slice_min(order_by = MSE, with_ties = FALSE)
best.lag
# best lag is 17

covid.test.tbl %>%
  mutate(cases.lag = lag(covid.test.tbl$cases.100k, 17)) %>%
  filter(!is.na(cases.lag)) %>%
  ggplot(aes(x = cases.lag, y = deaths.100k)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

