---
title: "Linear regression and tidymodels"
author: "Jaime Davila"
date: "2/27/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
```



classification
  response variable is categorical/discrete. 

prediction
  linear model: $y = a0 + a1*x + a2*x + ...$
  goal: minimize sum of residual squares: sum (y - y')^2
  similar to mse where you divide by n. Here, you don't divide b n. 

Tidy models: 
  this







# Introduction to linear regression using `tidymodels`

In today's class we will explore concepts about linear regression and how they are implemented using the `tidymodels` package. The `tidymodels` packages allows for a consistent interface to explore a multitude of prediction models so we will start learning its syntax using linear models as an example.

## Loading the dataset and creating testing and training data

We can load this package by making use of the following code. Notice that we use the function `tidymodels_prefer()` which allows to resolve known conflicts between different packages with the `tidymodels` package.

```{r}
library(tidymodels)
tidymodels_prefer()
```

The example that we will be using today will be the `ames` dataset which is available in the `modeldata` package. The `ames` dataset contains information about real estate prices in Ames, Iowa. Let's load the dataset and check its size:

```{r echo=TRUE}
library(modeldata)
data(ames)
dim(ames)
ames
```
many variables 
predict price of real estate based on features of real estate

1. Do a histogram of the `Sale_Price` variable. What do you notice? Create a new variable `log_Sale_Price` which corresponds to the log10 of the `Sale_Price`. How does the histogram of the new variable look like?
```{r}
ggplot(ames, aes(x = Sale_Price)) + 
  geom_histogram()

ames <- ames %>%
  mutate(log_Sale_Price = log(Sale_Price))
ames %>%
  ggplot(aes(x = log_Sale_Price)) + 
  geom_histogram()

```
It looks similar to the original variable Sale_Price but it is further up the x-axis.

2. The first task in the creation of a model is to divide the dataset into testing and training. Create an initial split of the data using the function `initial_split` from `tidymodels` with 80% of the data in your training dataset and 20% in your testing. How does the distribution of the response variable (`log_Sale_Price`) look across training and testing datasets? Please leave the command `set.seed(12345)` which allows the results to be reproducible.

```{r}
set.seed(12345)
x <- initial_split(ames, prop = 4/5)
trainTbl1 <- training(x)
testTbl1 <- testing(x)

trainTbl1 %>%
  ggplot(aes(x = log_Sale_Price)) + 
  geom_histogram()
  
testTbl1 %>%
  ggplot(aes(x = log_Sale_Price)) + 
  geom_histogram()
```
The testing dataset has much lower count of values so it is more spread-out.

## Creating linear models with `parsnip`

We are interested in predicting the `log_Sale_Price` as a linear of `Longitude` and `Latitude`. In order to do that we will be using `parsnip` which provides a standardized interface to create models. 

The first part consist of declaring the type of model we will be using (linear regression using `linear_reg`) and what implementation (or engine) of linear regression we will be using (`lm`). We can do do this using the following code:

```{r echo=TRUE}
lm.model <- linear_reg() %>%
  set_engine("lm")
lm.model
```

In the next part we declare a workflow, which is a set of steps where preprocessing and modeling are glued together. In our case we are not doing any preprocessing, so notice we are just adding our linear model (`lm.model`) and the formula for the model (`log_Sale_Price~Longitude + Latitude`) in the following code

```{r echo=TRUE}
lm.wflow <- workflow() %>%
  add_model(lm.model) %>%
  add_formula(log_Sale_Price ~ Longitude + Latitude)
```

Finally, we fit our workflow using our training dataset:

```{r echo=TRUE}
ames.train <- trainTbl1
lm.fit <- fit(lm.wflow, ames.train)
lm.fit
```

Notice that this set of steps corresponds to the second step of our class, namely fitting the model using our testing dataset. For completeness let's include all of the steps in a single chunk:

```{r echo=TRUE}
lm.model <- linear_reg() %>%
  set_engine("lm")

lm.wflow <- workflow() %>%
  add_model(lm.model) %>%
  add_formula(log_Sale_Price ~ Longitude + Latitude)

lm.fit <- fit(lm.wflow, ames.train)
```

3 steps:
training/testing
model build on training
evaluate on testing and calculate mse

3. We can extract information about the linear model using the command `extract_fit_engine()` piped with either `tidy()` or `glance`. Extract the information about the coefficients of your linear model. Are houses in North Ames usually more expensive that in the South? How about East-West?  What is the $R^2$ value of your model?

northfield: 
latitude is 44.45, longitutde is 93.16
duluth:
latitude is 46.75 north, longitude is 92.10 west
nebraska:
latitude is 43.62
longitude is 95.59


```{r}
lm.fit %>% extract_fit_engine() %>% tidy()
lm.fit %>% extract_fit_engine() %>% glance
```
r^2 = 0.173
North - South: + Longitude: decrease in price (East = expensive)
East - West: + Latitude: increase in price (North = expensive)

4. Use the `predict` command using your `lm.fit` and your testing dataset. What is the type of the output of `predict`? Consult the help for the command `augment` from `tidymodels` and use it to create a new column in your testing dataset with the prediction of your model. Do a histogram comparing your predictions with your response variable. What can you conclude from this graph?

```{r}
testTbl1
lmPredict <- lm.fit %>% predict(testTbl1)# output type is a tibble

testTbl2 <- augment(lm.fit, testTbl1, predict(testTbl1))$.pred

testTbl2 %>%
  ggplot(aes(x = sales_price_log)) + 
  geom_histogram()
```
answer:
```{r}
```
note solutions from in-class

5. Consult the documentation for `rmse` and `rmse_vec` from the the `yardstick` package. What are the types of the outputs for each function? Use this information to compute the MSE of your model on your training dataset.


