---
title: "Using recipes in tidymodels"
author: "Jaime Davila"
date: "3/1/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

notes:
  we did prediction and classification
  x1, x2, x... --> predict y (y is continuous)
    one of the most fundamental ways to do prediction is through a linear model. equation is linear combination (ax + a1x + ...)
      the best line minimizes error (misclassification rate)
      "residual error" = RSS = sum of (predicted - actual)
      note: RSS = n*MSE
      TSS = sum of(predicted - actual_mean)^2 = (n*variance)
      r^2 = (TSS - RSS) / TSS
    
lm()
knn3()
knnreg()
I think this is what these 3 are called. see how to use these 3.
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
library(dslabs)
```

# Using recipes in  `tidymodels`

In today's class we will explore how to create better models by creating new variables from old ones. This process is sometimes called *feature engineering* and we will learn how to do using `recipes` from `tidymodels`. In this worksheet we will be covering most of the material from https://www.tmwr.org/recipes.html 

## Using the ames dataset

Let's start by loading the appropriate libraries, datasets, and converting our variable `price` so that is in on log-scale. Also let's make sure that we create our testing and training dataset

```{r echo=TRUE}
library(tidymodels)
tidymodels_prefer()

library(modeldata)
data(ames)
ames <- ames %>%
  mutate(Sale_Price=log10(Sale_Price))

set.seed(12345)
ames.split <- initial_split(ames, prop=0.8)
ames.train <- training(ames.split)
ames.test <- testing(ames.split)
```

## Initial modeling

We are interested in predicting the price of the property adding  the following variables:

* `Neighborhood`

* `Gr_Liv_Area`, corresponding to the gross above-grade living area.

* `Year_built`

* `Bldg_type` corresponding to the building type

1. What is the type of each of these four variables? If a variable is categorical how many different values (levels) it has

```{r}
#ames.test
class(ames.train$Neighborhood)
class(ames.train$Gr_Liv_Area)
class(ames.train$Year_Built)
class(ames.train$Bldg_Type)

levels(ames.train$Neighborhood)#  29 levels
levels(ames.train$Bldg_Type)#   5 levels

unique(ames.train$Neighborhood)
#unique(ames.train$Gr_Liv_Area)#  this is messy
```
factor int int factor
typeof() just calls them all ints. (levels are stored as integers)

2. Do a histogram of `Gr_Liv_Area`. How does this histogram looks using a log scale?
```{r}
ames.train %>%
  ggplot(aes(x = Gr_Liv_Area)) +
    geom_histogram()

ames.train %>%
  mutate(Gr_Liv_Area = log(Gr_Liv_Area)) %>%
  ggplot(aes(x = Gr_Liv_Area)) +
    geom_histogram()
```
Log scale histogram looks more normally distributed

## Creating a recipe

A recipe is a collection of steps for preprocessing a dataset. Our initial recipe will include the following steps:

* We would like to make it explicit that we are modeling the `Sale_Price` (response variable) based on `Latitude` and `Longitude`, `Gr_Liv_area`, and `Bldg_type` (explanatory variables)

* We would like to use a log scale for `Gr_Liv_Area`

* We would like to transform all of our categorical variables into indicator variables.

```{r echo=TRUE}
ames.recipe <- 
  recipe(Sale_Price ~ Longitude + Latitude + Gr_Liv_Area + 
            Bldg_Type, data=ames.train) %>%
  step_log(Gr_Liv_Area, base=10) %>%
  step_dummy(all_nominal_predictors())#turn all nominal variables into factors?
ames.recipe
```

Once we created the recipe we can use in conjunction with a linear model, add it to a workflow and fit our workflow using our training dataset

```{r echo=TRUE}
lm.model <- linear_reg() %>%
  set_engine("lm")

lm.wflow <- workflow() %>%
  add_recipe(ames.recipe) %>%
  add_model(lm.model) 

lm.fit <- fit(lm.wflow, ames.train)
lm.fit
```

3. 
a. What is the $R^2$ of the linear model you created? How do you interpret this value?

```{r}
lm.fit %>%
  extract_fit_engine() %>%
  glance()
```
r^2 = 0.610
R^2 = $(n*variance - sum(predicted - actual) ) / n*variance$
r^2 represents how well the model works compared with actual data. 
1 is a perfect model. 
"This model accounts for 61% of the variance of this graph"

b. Interpret the coefficients corresponding to:

```{r}
lm.fit %>%
  extract_fit_engine() %>%
  tidy()
```

* The living area of the house.

for every increase by log-scale in 1, there is increase in log-scale of .846 in log scale for price

* The type of building.

  log(price) decreases for 3 of building types by a log constant for each property (-.133, -.115, or -.0414)
  and increases for one: TwnhsE by 0.0598
  

4. Evaluate your model using the testing dataset. What is the MSE on this dataset?
```{r}
vals <- predict(lm.fit, ames.test)
vals
mse.test <- mean ((ames.test$Sale_Price - vals$.pred)^2)
mse.test
```
[1] 0.0112031


prof:
new.ames.test <- lm.fit %>%
  augment(new_data = ames.test)
rmse_vec(new.ames.test$Sale_Price, new.ames.test$pred)^2
0.112031


5. Add `Year_Built` as an input variable in your existing recipe. What is the $R^2$ of your model? What is the MSE on the testing dataset?
```{r echo=TRUE}
ames.recipe.no5 <- 
  recipe(Sale_Price ~ Longitude + Latitude + Gr_Liv_Area + 
            Bldg_Type + Year_Built, data=ames.train) %>%
  step_log(Gr_Liv_Area, base=10) %>%
  step_dummy(all_nominal_predictors())#turn all nominal variables into factors?
ames.recipe.no5
```
```{r echo=TRUE}
lm.model.no5 <- linear_reg() %>%
  set_engine("lm")

lm.wflow.no5 <- workflow() %>%
  add_recipe(ames.recipe.no5) %>%
  add_model(lm.model.no5) 

lm.fit.no5 <- fit(lm.wflow.no5, ames.train)
lm.fit.no5
```

```{r}
lm.fit.no5 %>%
  extract_fit_engine() %>%
  glance()
```
R^2 = 0.733

```{r}
vals.no5 <- predict(lm.fit.no5, ames.test)
vals.no5
mse.test.no5 <- mean((ames.test$Sale_Price - vals.no5$.pred)^2)
mse.test.no5
```
mse = 0.007296907

6. Add `Neighborhood` as an input variable recipe to your model from 5. What is the $R^2$ of your model? What is the MSE on the testing dataset?

```{r echo=TRUE}
ames.recipe.no6 <- 
  recipe(Sale_Price ~ Longitude + Latitude + Gr_Liv_Area + 
            Bldg_Type + Year_Built + Neighborhood, data=ames.train) %>%
  step_log(Gr_Liv_Area, base=10) %>%
  step_dummy(all_nominal_predictors())#turn all nominal variables into factors?
ames.recipe.no6
```
```{r echo=TRUE}
lm.model.no6 <- linear_reg() %>%
  set_engine("lm")

lm.wflow.no6 <- workflow() %>%
  add_recipe(ames.recipe.no6) %>%
  add_model(lm.model.no6) 

lm.fit.no6 <- fit(lm.wflow.no6, ames.train)
lm.fit.no6
```

```{r}
lm.fit.no6 %>%
  extract_fit_engine() %>%
  glance()
```
R^2 = 0.801

```{r}
vals.no6 <- predict(lm.fit.no6, ames.test)
vals.no6
mse.test.no6 <- mean((ames.test$Sale_Price - vals.no6$.pred)^2)
mse.test.no6
```
MSE = 0.005853077

7. 
a. Summarize and sort the number of observations in each neighborhood. How many many neighborhoods have less than 20 observations? 

```{r}
ames %>%
  mutate(row = row_number()) %>%
  select(row, Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarize(observations = n()) %>%
  filter(observations < 20)
```
There are four such neighborhoods.
Blueste, Greens, Green_Hills, and Landmark.

b. Consult the documentation for `step_other` and add a step to your recipe where you collapse neighborhoods with less than 1% of your data. Make sure to add this step before the `step_dummy` command.

```{r echo=TRUE}
ames.recipe.no7 <- 
  recipe(Sale_Price ~ Longitude + Latitude + Gr_Liv_Area + 
            Bldg_Type + Year_Built + Neighborhood, data=ames.train) %>%
  step_log(Gr_Liv_Area, base=10) %>%
  step_other(Neighborhood, threshold = 0.01) %>%
  step_dummy(all_nominal_predictors())#turn all nominal variables into factors?
ames.recipe.no7
```
```{r echo=TRUE}
lm.model.no7 <- linear_reg() %>%
  set_engine("lm")

lm.wflow.no7 <- workflow() %>%
  add_recipe(ames.recipe.no7) %>%
  add_model(lm.model.no7) 

lm.fit.no7 <- fit(lm.wflow.no7, ames.train)
lm.fit.no7
```

c. Rerun your workflow and your model. How do you interpret the coefficient of the model associated with the collapsed set of neighborhoods? What is the MSE of this new model?

```{r}
output <- lm.fit.no7 %>%
  extract_fit_engine() %>%
  tidy()
print(output, n = 31)
```
Neighborhoods with fewer listings tend to have a higher sale price of log-inverse of 5.94e-2.

```{r}
vals.no7 <- predict(lm.fit.no7, ames.test)
vals.no7
mse.test.no7 <- mean((ames.test$Sale_Price - vals.no7$.pred)^2)
mse.test.no7
```
mse = 0.005911335

8. 
a. What two features are you planning to use for your first challenge?

An idea: 
x_1 = abs(q1 - q4)
x_2 = abs(q2 - q3)

b. Using the MNIST dataset select a couple of instances of the two digits assigned to your group. Calculate the two features on those instances. Are the two features similar across the two different types of digits?

The numbers of my group are 0 and 7. 0 is symmetric so we should measure with differences between halves or quarters. 

I am copy-pasting my work from the project document:

```{r}
mnist <- read_mnist("~/Mscs 341 S22/Class/Data")
str(mnist)
```
```{r}
set.seed(12345)
index <- sample(1:60000, 60000)
#index
tester <- as_tibble(index) %>%
  #mutate(row = row_number()) %>%
  mutate(image = mnist$train$images[value, ]) %>%
  mutate(label = mnist$train$labels[value]) %>%
  dplyr::filter(label == 0 | label == 7)
tester$image[10,]
tester$label[10]
```
```{r}
tester <- tester %>%
  slice(1:1000)
set.seed(12345)
trainer <- slice_sample(tester, n = 800)
tester <- setdiff(tester, trainer)
tester
```

First, take a single vector of 0: (will also do this process for 7 as a comparison: tester$label[1])
```{r}
zeroMatrix <- matrix(trainer$image[4, ],nrow=28)[,28:1]
sum(zeroMatrix[1:14, 15:28])#q1
sum(zeroMatrix[1:14, 1:14])#q2
sum(zeroMatrix[15:28, 1:14])#q3
sum(zeroMatrix[15:28, 15:28])#q4

#visually, the matrix kind of looks like a zero if you visually piece them together. I think it was divided correctly.

abs(sum(zeroMatrix[1:14, 15:28]) + sum(zeroMatrix[15:28, 1:14]) - sum(zeroMatrix[1:14, 1:14]) - sum(zeroMatrix[15:28, 15:28]))
```
q1 = 6588;    q2 = 8456;    q3 = 7982;    q4 = 8562
x_1 = abs(q1 - q4) = 1974
x_2 = abs(q2 - q3) = 474

Now, try this process above for 7:
```{r}
sevenMatrix <- matrix(trainer$image[1, ],nrow=28)[,28:1]
sum(sevenMatrix[1:14, 15:28])#q1
sum(sevenMatrix[1:14, 1:14])#q2
sum(sevenMatrix[15:28, 1:14])#q3
sum(sevenMatrix[15:28, 15:28])#q4

abs(sum(sevenMatrix[1:14, 15:28]) + sum(sevenMatrix[15:28, 1:14]) - sum(sevenMatrix[1:14, 1:14]) - sum(sevenMatrix[15:28, 15:28]))
```
q1 = 4821;    q2 = 1757;    q3 = 7921;    q4 = 4487
x_1 = abs(q1 - q4) = 334
x_2 = abs(q2 - q3) = 6164

The results for the two numbers are fairly different. (plotting a bunch of data points would be a better indicator to guage if x_1 and x_2 are reasonable variables to use to classify 0 and 7.)

