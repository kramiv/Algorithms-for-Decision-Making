---
title: "ADM Sample Exam 1"
author: "Jaime Davila"
date: "3/21/22"
output:
  pdf_document: default
  html_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---
\newcommand{\pledgeline}{
\setlength{\unitlength}{1in}
\begin{picture}(6, .125)
\put(0,0){\line(1,0){4}}
\end{picture}
}

\newcommand{\ret}{
\vspace{1cm} }
\newcommand{\reta}{
\vspace{.5cm} }
\newcommand{\retb}{
\vspace{2.5cm} }
\newcommand{\retc}{
\vspace{3.2cm} 
}


```{r setup warning=FALSE, message=FALSE}
knitr::opts_chunk$set(cache = TRUE)
library(tidyverse)
library(tidymodels)
library(kknn)
library(dslabs)
library(discrim)
tidymodels_prefer()
```

### Exam 1 Guidelines

- You have the entire class time (80 minutes) to complete this exam.  

- Please make sure to save *immediately* your work in your submit folder. By the end of class time save the last version of your work and *don't* modify it afterwards.

- You are to work completely independently on the exam. 

- You are allowed to use your class notes, moodle, worksheets, homeworks, textbooks, plus the "Help" feature from Rstudio.

- You **are not** permitted to do web searches.

- Please silence your cell phone.  Place it and any other connected devices in your pack and do not access them for any reason.


For questions that ask for interpretations and explanations, usually no more than a sentence or two is needed for justification.  Be thorough, but do not “brain dump”.  Notice that three sections of this exam are independent and that you can complete later sections successfully whether or not earlier sections are correct. 

Do not spend too long on any one question. If you are not sure about an answer, write a note detailing your concern.


**PLEDGE:**  I pledge on my honor that I have neither received nor given assistance during this exam nor have I witnessed others receiving assistance, and I have followed the guidelines as described above.  

\vspace{3mm}

**SIGNATURE:** \pledgeline

$\bigcirc$ I have intentionally not signed the pledge.

\vspace{3mm}


# Introduction

The dataset from The Pudding ["Baking the Most Average Chocolate Chip Cookie"](https://pudding.cool/2018/05/cookies/) collects over 200 recipes of chocolate chip cookies and their popularity. Let's start by loading the dataset using:

```{r warning=FALSE, message=FALSE}
url <- "https://raw.githubusercontent.com/the-pudding/data/master/cookies/choc_chip_cookie_ingredients.csv"
recipe.raw.tbl <- read_csv(url)
```

Here is some brief information about the columns:

* Ingredient: The name of the ingredient used for a chocolate chip cookie.
* Text: The text obtained from the recipe that makes reference to the ingredient.
* Recipe_Index: A unique identifier associated with each recipe.
* Rating: The online rating of the recipe.
* Quantity: How much of a particular ingredient is required.
* Unit: The units in which the ingredient is measured.

# Wrangling (20 points)

Before we start our analysis we need to clean and make sure our dataset is in the right shape. We will divide that into the following steps:

## Step One (5 points)

Create a tibble `recipe.wide.tbl` with columns representing each ingredient and rows for each  recipe. Make sure that in case of duplicate entries you take the mean of the multiple entries.
*Hint:* Make sure to use the parameters `values_fn` from your pivoting function.
```{r}
recipe.wide.tbl <- recipe.raw.tbl %>%
  select(Recipe_Index, Ingredient, Rating, Quantity) %>%
  pivot_wider(names_from = "Ingredient", values_from = Quantity, values_fn = mean)
```

## Step two (5 points )

Describe in your own words what the following code does. *Do not* describe what each individual line of code does, but what is the overall idea that the code is implementing.


```{r echo=TRUE}
recipe.filter.tbl <- recipe.wide.tbl %>%
  filter(!is.na(Rating)) %>%
  select(Recipe_Index, Rating, `all purpose flour`, `semisweet chocolate chip`, 
         "butter", "sugar") %>%
  rename("flour"=`all purpose flour`,
         "chocolate"=`semisweet chocolate chip`) %>%
  filter(!(is.na(flour) | is.na(chocolate) | is.na(butter) | is.na(sugar)))
```


## Step Three (5 points)

Create a new tibble `recipe.final.tbl` that has new variables representing the proportions of flour, chocolate, butter and sugar in the recipe (name your variables `prop.flour`, `prop.chocolate`, `prop.butter` and `prop.sugar`)  
```{r}
recipe.final.tbl <- recipe.filter.tbl %>%
  group_by(Recipe_Index) %>%
  mutate(total = sum(flour, chocolate, butter, sugar)) %>%
  mutate(prop.flour = flour/total, prop.chocolate = chocolate/total, prop.butter = butter/total, prop.sugar = sugar/total)
recipe.final.tbl
```

## Step Four (5 points)

Create a boxplot representing the proportions of the four main ingredients from your dataset (flour, chocolate, butter and sugar).
```{r}
recipe.final.tbl %>%
  pivot_longer(prop.flour:prop.sugar, names_to = "type.prop", values_to = "type.prop.vals") %>%
  ggplot() +
    geom_boxplot(aes(type.prop, type.prop.vals))
```

# Modelling (40 points)

Make sure that the tibble you obtained from your wrangling has 55 observations and 11 variables. In case you ran into problems you can use the following code for the next points:

```{r echo=TRUE}
recipe.tbl <-  tibble(read.table("~/Mscs 341 S22/Class/Data/recipe.Rdata"))
dim(recipe.tbl)
```


## Step One (10 points)

We are interested in forecasting how good a recipe will be based on the proportion of the  key ingredients (flour,chocolate, butter). What are your input and response variables? Is this a classification or prediction problem?

input = proportion of flour, chocolate, butter, sugar. (numerical)

response = rating (numerical)

prediction

## Step Two (20 points)

We start by dividing our dataset into testing and training as follows

```{r echo=TRUE}
#Training/Testing dataset
set.seed(54321)
recipe.split <- initial_split(recipe.tbl)
recipe.train.tbl <- training(recipe.split)
recipe.test.tbl <- testing(recipe.split)
```

We would like to use `tidymodels()` to create a KNN model to forecast the rating based on the proportion of flour, chocolate and butter . Using 10-fold cross validation, plot the `rmse` across `neighbors=`$5,10,\dots,30$ and use the plot to find the optimal number of neighbors. In your own words in a paragraph or two, describe how the 10-fold cross validation process works on this instance.

KNN model
predict rating - numerical.
10-fold cross validation


```{r}
set.seed(54321)
knn.model <- nearest_neighbor(neighbors = tune()) %>%
set_engine("kknn") %>%
set_mode("regression")
recipe <- recipe(Rating ~ prop.flour + prop.chocolate + prop.butter, data=recipe.train.tbl)
knn.wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(knn.model)
# Neighbors optimization using CV
```

```{r}
recipe.folds <- vfold_cv(recipe.train.tbl, v = 10)
neighbors.grid.tbl <- tibble(neighbors = seq(5,30, by=5))
tune.results <- tune_grid(object = knn.wf,
resamples = recipe.folds,
grid = neighbors.grid.tbl)
autoplot(tune.results, metric="rmse")
```


## Step Three (10 points)

The $R^2$ is the squared correlation between the response variable and the model's prediction and can be calculated using the `rsq()` function. Using your results from `Step Two` select the optimal number of neighbors based on the $R^2$ and evaluate the $R^2$ on the testing dataset. How does it compare to the $R^2$ that you calculated using cross-validation? 

```{r}
show_best(tune.results, metric="rsq")
```
```{r}
best.neighbor <- select_best(tune.results,
neighbors, metric = "rsq")# this is 20
best.neighbor

knn.final.wf <- finalize_workflow(knn.wf, best.neighbor)
knn.final.fit <- fit(knn.final.wf, recipe.train.tbl)
# Evaluation of model on the testing dataset
augment(knn.final.fit, recipe.test.tbl) %>%
rsq(truth = Rating, estimate = .pred)

```


# Aiming for the Stars (40 points)

We will be using the `stars` dataset from the `dslabs` package. More information on this dataset can be found using `?stars`

```{r echo=TRUE}
data(stars)
stars.tbl <- tibble(stars) 
```

## Part one (15 points)

Subset your dataset to only stars of type A,B,K and M. For each star, plot the magnitude and temperature on the plane and use a different color for each star type. Based on this information, would an LDA or a QDA provide a better model?

```{r}
star.levels = c("A","B","K","M")
stars.filter.tbl <- stars.tbl %>%
filter(type %in% star.levels) %>%
mutate(type = factor(type, levels=star.levels))
ggplot(stars.filter.tbl, aes(magnitude, temp,
color=type, shape=type))+
geom_point()
```

## Part two (15 points)

As usual let's create our training/testing dataset
```{r echo=TRUE}
#Training/Testing dataset
set.seed(12345)
stars.split <- initial_split(stars.filter.tbl)
stars.train.tbl <- training(stars.split)
stars.test.tbl <- testing(stars.split)
```

Implement an LDA and a QDA model that would predict the type of star and evaluate on your testing dataset. Based on the confusion matrix what model does a better job? Explain briefly why.

```{r}
lda.model <- discrim_linear() %>%
set_engine("MASS") %>%
set_mode("classification")
recipe <- recipe(type ~ magnitude+temp, data=stars.train.tbl)
lda.wflow <- workflow() %>%
add_recipe(recipe) %>%
add_model(lda.model)
lda.fit <- fit(lda.wflow, stars.train.tbl)
augment(lda.fit,stars.test.tbl) %>%
conf_mat(type, .pred_class)

```

## Part three (10 points)

Create a two dimensional grid with around 10,000 points and plot the predicted class for each model (lda and qda). How do the boundary regions differ for each model?

```{r}
grid.tbl <- expand_grid(magnitude=seq(-10,20, by=0.3),
temp = seq(0,35000, by=350))
augment(lda.fit, grid.tbl) %>%
ggplot(aes(magnitude,temp, fill=.pred_class)) +
geom_raster()
```

```{r}
augment(qda.fit, grid.tbl) %>%
ggplot(aes(magnitude,temp, fill=.pred_class)) +
geom_raster()

```

