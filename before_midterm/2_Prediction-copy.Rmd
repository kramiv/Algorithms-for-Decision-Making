---
title: "Prediction"
author: "Ivana K."
date: "2/16/2021"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
```

# Notes:
"Prediction" means the response variable is continuous





# Classification and choice of K

On a first instance we will be exploring our Minneapolis police incidents dataset in a more systematic way and construct visualizations that explore the effect of the value of $K$ in our KNN model.

Let's start by loading the dataset

```{r}
mn.police.tbl <- read_csv("~/Mscs 341 S22/Class/Data/police_incidents.mn.csv")
head(mn.police.tbl)
```

And remember the steps that we took last time, namely:

1. Divide our dataset into training and testing datasets.
2. Construct a model based on the training dataset.
3. Evaluate the fit of the model by calculating the MSE on the testing dataset

These steps can be done as follows:

```{r}
# Divide dataset into testing and training
train.mn.police.tbl <- mn.police.tbl %>%
  filter(year==2016)
test.mn.police.tbl <- mn.police.tbl %>%
  filter(year==2017)

# Build the KNN model
kNear=7
knn.model <- knnreg(tot~week, data=train.mn.police.tbl,k=kNear)

# Evaluate the MSE of the KNN model in the testing dataset
test.pred <- predict(knn.model, test.mn.police.tbl)
(mse.test <- mean ((test.mn.police.tbl$tot-test.pred)^2))
```

We are interested in calculating systematically the MSE as we iterate over the parameter $k$ by doing the following steps: 

1. Create a function `calc_MSE(kNear, train.tbl, test.tbl)` that trains a KNN model with parameter `kNear` on `train.tbl` and then applies the model on `test.tbl` and calculates the MSE. Test your function with k=7 and k=35 using our testing and training datasets.

```{r}
calc_MES <- function(kNear, train.tbl, test.tbl) {
  #train KNN model on train.tbl
  knn.model <- knnreg(tot~week, data=train.tbl,k=kNear)
  #apply to test.tbl
  test.pred <- predict(knn.model, test.tbl)
  #calculate MSE
  (mse.test <- mean ((test.tbl$tot-test.pred)^2))
}

calc_MES(7,train.mn.police.tbl, test.mn.police.tbl)
calc_MES(35,train.mn.police.tbl, test.mn.police.tbl)
```

2. We would like to create a vector with the MSE values for our testing dataset. Notice it only makes sense to look at values of k in increments of 7 (why?). Use a for loop in R(Look at the syntax of for loops in https://rafalab.github.io/dsbook/programming-basics.html#for-loops) to create the mse for $k=7,14,21,...,364$

```{r}
mseVector <- function(startVal, endVal) {
  vector = c()
  for(i in seq(from=startVal, to=endVal, by=7)){
  vector[i/7] = calc_MES(i,train.mn.police.tbl, test.mn.police.tbl)
  }
  vector
}
testDataSet <- mseVector(7,364)

mseVector <- function(startVal, endVal) {
  vector = c()
  for(i in seq(from=startVal, to=endVal, by=7)){
  vector[i/7] = calc_MES(i,train.mn.police.tbl, train.mn.police.tbl)
  }
  vector
}
trainDataSet <- mseVector(7,364)
```
"Notice it only makes sense to look at values of k in increments of 7 (why?)" not sure.

3. Generate a graph depicting the MSE as a function of `k` for both testing and training datasets. What is the optimal value for `k` based on the testing dataset? Can you find this value in a systematic way? (*Hint*: Check the documentation for function `slice_min`). Compare this graph against figures 2.9 and 2.10 from your book. What would be the equivalent of the parameter `flexibility` in your KNN model?

generate graph:
```{r}
testDataSet
trainDataSet
kVals = c()
for(i in seq(from=7, to=364, by=7)){
  kVals[i/7] = i
}
kVals
```

plot it and get into tibble:
```{r}
finally <- as_tibble(data.frame(kVals, testDataSet, trainDataSet))
finally2 <- finally %>%
  mutate(diffVal = testDataSet - trainDataSet) %>%
  pivot_longer(cols = testDataSet:trainDataSet, names_to = "type", values_to = "MESVal")

finally2 %>%
  mutate(type = as.factor(type)) %>%
  ggplot(mapping = aes(kVals, MESVal)) +
  geom_line(mapping = aes(kVals, MESVal, color = type))
```

find value of k in systematic way. use slice_min
```{r}
finally2

finally2 %>%
  filter(type == "testDataSet") %>%
  slice_min(MESVal, with_ties = FALSE)
```
read plot on figure 2.9 in book. and the thing with flexibility
It should be 217. why?

# Improving your model

One way to improve the performance of a model is to use the information provided by other variables. In the following exercises we will explore this in more detail:

(day of the week)

4. Does the distribution of number of incidents across the days of the week? Generate a boxplot to explore this question and check if this behavior is consistent across the years

```{r}
days = unique(mn.police.tbl$wday) 

mn.police.tbl %>%
  mutate(year = as.factor(year), 
         wday = factor(wday, levels = days)) %>%#factor puts it in certain order
    ggplot(aes(x = wday, y = tot, 
               color = year)) +
      geom_boxplot()
```

5. It seems police incidents are higher on Monday as opposed to other days. Subset you dataset to only Mondays and construct a KNN model using `week` as input variable and train it using the data from 2016. Plot a graph with the MSE for all different choices of $k$ and select a $k$ that minimizes the MSE on the testing. Is the MSE smaller using this model than our original model?

```{r}
# Divide dataset into testing and training
days = unique(mn.police.tbl$wday) 

trainDayTbl <- mn.police.tbl %>%
  mutate(year = as.factor(year), 
         wday = factor(wday, levels = days)) %>%
  filter(year == 2016) %>%#   train KNN using data from 2016
  filter(wday == "Mon")#  subset data to only include Mondays

trainDayTbl

testDayTbl <- mn.police.tbl %>%
  mutate(year = as.factor(year), 
         wday = factor(wday, levels = days)) %>%
  filter(year == 2017) %>%
  filter(wday == "Mon")

testDayTbl
```

```{r}
# Build the KNN model
kNear=14
knn.model2 <- knnreg(tot~week, data=trainDayTbl,kNear)#week as input variable

# Evaluate the MSE of the KNN model in the testing dataset
test.pred2 <- predict(knn.model2, testDayTbl)
(mse.test <- mean((testDayTbl$tot-test.pred2)^2))
```

put stuff into function so a vector of MES values can be calculated:
```{r}
calc_MES2 <- function(kValue, traintable, testtable) {
  #train KNN model on train.tbl
  knn.model2 <- knnreg(tot~week, data=traintable,k=kValue)
  #apply to test.tbl
  test.pred2 <- predict(knn.model2, testtable)
  #calculate MSE
  (mse.test <- mean ((testtable$tot-test.pred2)^2))
}

#calc_MES2(7,trainDayTbl, testDayTbl)#138.6318
#calc_MES2(35,trainDayTbl, testDayTbl)
```

```{r}
#test
mseVector <- function(startVal, endVal) {
  vector = c()
  for(i in seq(from=startVal, to=endVal)){#, by=7
  vector[i] = calc_MES2(i,trainDayTbl, testDayTbl)#[i/7]
  }
  vector
}
testDataSet2 <- mseVector(7,52)
#testDataSet2

#train
mseVector <- function(startVal, endVal) {
  vector = c()
  for(i in seq(from=startVal, to=endVal)){
  vector[i] = calc_MES2(i,trainDayTbl, trainDayTbl)
  }
  vector
}
trainDataSet2 <- mseVector(7,52)
#trainDataSet2
```
generate graph:
```{r}
kVals2 = c()
for(i in seq(from=7, to=52)){#, by=7
  kVals2[i] = i#[i/7]
}
#kVals2
```
get into tibble for plotting:
```{r}
tibble1 <- as_tibble(data.frame(kVals2, testDataSet2, trainDataSet2))
tibble2 <- tibble1 %>%
  mutate(diffVal = testDataSet2 - trainDataSet2) %>%
  pivot_longer(cols = testDataSet2:trainDataSet2, names_to = "type", values_to = "MESVal")
#tibble2

tibble2 %>%
  mutate(type = as.factor(type)) %>%
  filter(type == "testDataSet2") %>%
  ggplot(mapping = aes(kVals2, MESVal)) +
  geom_line(mapping = aes(kVals2, MESVal, color = type))
```

find value of k in systematic way. use slice_min
```{r}
tibble2 %>%
  filter(type == "testDataSet2") %>%
  slice_min(MESVal, with_ties = FALSE)
```
original: 
  kVals diffVal type        MESVal
  <dbl>   <dbl> <chr>        <dbl>
1   217    27.8 testDataSet   145.

MSE value is smaller in this model.

# Studying the COVID pandemic

For the next set of exercises we will be using the US covid dataset procured from [CovidActNow](covidactnow.org). We'll start by loading the dataset. Notice that I also loaded the library `lubridate` which allows for convenient use of dates.

```{r}
library(lubridate)
covid.tbl <- read_csv("~/Mscs 341 S22/Class/Data/covid.csv")
```

6. Subset your dataset from August 2020 to August 2021 and plot the median number of cases and the median number of deaths (per 100,000). *Hint*: You can filter dates using `filter` and you will need to create a reference date with `as.Date` 
```{r}
covid.tbl
covid.tbl %>%
  filter(date >= as.Date("2020-08-01")) %>%
  filter(date <= as.Date("2021-08-31")) %>%
  group_by(date) %>%
  summarize(med.cases.100k = median(cases.100k),
            med.deaths.100k = median(deaths.100k)) %>%
  pivot_longer(med.cases.100k:med.deaths.100k, names_to = "type", values_to = "median.vals") %>%
  mutate(type=as.factor(type)) %>%
  ggplot() +
    geom_line(mapping = aes(date, median.vals)) +
    facet_wrap(~ type, ncol = 1) +
    scale_y_continuous(trans='log10')
```

7. We would like to create a model that would be able to predict the number of deaths based on the number of cases. To do that let's create training and testing datasets from neighboring states, let's say WI and MN. Plot the number of cases against the number of deaths for your training dataset and include a linear trend in your plot

```{r}
covid.train.tbl <- covid.tbl %>%#   train tbl: WI
  filter(state == "WI") %>%
  filter(date >= as.Date("2020-08-01")) %>%
  filter(date <= as.Date("2021-08-31"))

covid.test.tbl <- covid.tbl %>%
  filter(state == "MN") %>%
  filter(date >= as.Date("2020-08-01")) %>%
  filter(date <= as.Date("2021-08-31"))

covid.train.tbl %>%
  ggplot() +
    geom_point(mapping = aes(cases.100k, deaths.100k)) +
    geom_smooth(mapping = aes(cases.100k, deaths.100k), method='lm', se = TRUE)
```

8. Create a linear model using `lm()` using the data from WI (training) and evaluate how well it does in MN (testing).
```{r}
linear.model <- lm(deaths.100k~cases.100k, data=covid.train.tbl)
linear.model

covid.test.tbl <- covid.tbl %>%
  filter(state == "MN") %>%
  filter(date >= as.Date("2020-08-01")) %>%
  filter(date <= as.Date("2021-08-31"))

covid.test.tbl

test.pred.covid <- predict(linear.model, covid.test.tbl)
test.pred.covid
mse.test.covid <- mean((covid.test.tbl$deaths.100k-test.pred.covid)^2)
mse.test.covid
```
[1] 0.03737339

9. To improve our model, let's make use of the fact that the number of covid cases is a good predictor of the number of deaths a couple of weeks afterwards. Create a function `calc_MSE_lag(time.lag, train.tbl, test.tbl)` that calculates the MSE on the testing dataset by using a linear model where the deaths lag the number of cases by `time.lag` *Hint* Make use of the functions `lead/lag` from the tidyverse.

```{r}
calc_MSE_lag <- function(time.lag, train.tbl, test.tbl) {
  
  experimentTrain.tbl <- train.tbl %>%
    arrange(desc(date)) %>%
    mutate(deaths.100k = lag(deaths.100k, n = time.lag)) %>%
    filter(is.na(deaths.100k) == FALSE)
  
  
  experimentTest.tbl <- test.tbl %>%
    arrange(desc(date)) %>%
    mutate(deaths.100k = lag(deaths.100k, n = time.lag)) %>%
    filter(is.na(deaths.100k) == FALSE)

  linear.model <- lm(deaths.100k~cases.100k, data=experimentTrain.tbl)
  
  linear.model

  test.pred.covid <- predict(linear.model, experimentTest.tbl)
  
  test.pred.covid

  mse.test.covid <- mean((experimentTest.tbl$deaths.100k-test.pred.covid)^2)
  
  mse.test.covid
}

calc_MSE_lag(7, covid.train.tbl, covid.test.tbl)
calc_MSE_lag(14, covid.train.tbl, covid.test.tbl)
calc_MSE_lag(21, covid.train.tbl, covid.test.tbl)
```
> calc_MSE_lag(7, covid.train.tbl, covid.test.tbl)
[1] 0.02200948
> calc_MSE_lag(14, covid.train.tbl, covid.test.tbl)
[1] 0.01464344
> calc_MSE_lag(21, covid.train.tbl, covid.test.tbl)
[1] 0.01617032

10.  Plot lag versus MSE on the testing dataset and find the optimal parameter of lag. How do you interpret this optimal lag? Using this value of lag, plot the lagged number of cases versus the deaths and the linear trend line for the testing dataset.

```{r}
mseVectorCovid <- function(startVal, endVal) {
  vector = c()
  for(i in seq(from=startVal, to=endVal, by=1)){
  vector[i] = calc_MSE_lag(i, covid.train.tbl, covid.test.tbl)#/7
  }
  vector
}
testMseCovidVector <- mseVectorCovid(0,31)
testMseCovidVector
```

generate graph:
```{r}
kValsCovid = c()
for(i in seq(from=0, to=31, by = 1)){#, by=7
  kValsCovid[i] = i#[i/7]
}
kValsCovid
```
get into tibble for plotting:
```{r}
covidTibble <- as_tibble(data.frame(kValsCovid, testMseCovidVector))
covidTibble %>%
  ggplot(mapping = aes(kValsCovid, testMseCovidVector)) +
  geom_line(mapping = aes(kValsCovid, testMseCovidVector))
```

```{r}
covidTibble
```


```{r}
covidTibble %>%
  slice_min(testMseCovidVector, with_ties = FALSE)
```
kValsCovid testMseCovidVector
       <dbl>              <dbl>
1         17             0.0143

The optimal lag time predicted by the KNN model is 17 days (between the number of cases to the number of deaths that follow). 















