---
title: "Logistic regression in tidymodels"
author: "Jaime Davila"
date: "3/7/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---


note: echo = FALSE = removes code in output. 

prof notes on hw: 
idx <- mnist$train$label==1
test = mnist$train$images[idx,]
datatype of test is matrix
so to plot:
plotImage(test[10,])

prediction and classification
which are we doing today? classification.

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
```

# Introduction

Let's start by loading an old friend of ours, the `Default` dataset (remember Homework 3?)

```{r}
library(ISLR2)
data(Default)
default.tbl <- tibble(Default)
default.tbl
```

We are interested in predicting whether a person would go on *default* (that is, would not pay back a loan) given the following information:

* `balance` (How much does the person owe?)
* `income` (How much does the person earn?)
* `student` (Is the person a student?)

0. How many observations does this dataset have? How many defaults and non-defaults? How many defaults by student status?

```{r}
dim(default.tbl)
table(default.tbl$default)
table(default.tbl$default, default.tbl$student)
```

1. Create a boxplot of `balance` across people who default or not. What do you observe? What do you observe when you facet the boxplot by `student`?

```{r}
default.tbl %>%
  ggplot(aes(x = default, y = balance)) +
  geom_boxplot()

default.tbl %>%
  ggplot(aes(x = default, y = balance)) +
  geom_boxplot() +
  facet_wrap(~student)
```
People who default have a higher balance. This trend is very similar among students. Students just have a higher balance overall. 

2. How about the effect of `income` on defaulting? Does it change according to `student` status?

```{r}
default.tbl %>%
  ggplot(aes(x = default, y = income, fill = student)) +
  geom_boxplot()

default.tbl %>%
  ggplot(aes(x = default, y = income, fill = student)) +
  geom_boxplot() +
  facet_wrap(~student, ncol = 2)
```
Pretty much regardless of student status, people who default have a slightler lower income. 

3. Load `tidymodels` and create a training dataset using $8000$ observations and a testing dataset using $2000$ observations. Make sure to call your training dataset `default.train.tbl` and your testing dataset `default.test.tbl`

```{r echo=TRUE}
set.seed(12345)
```

```{r}
library(tidymodels)
tidymodels_prefer()
default.split <- initial_split(default.tbl, prop = 0.8)
default.train.tbl <- training(default.split)
default.test.tbl <- testing(default.split)
```

# Modeling probabilities with linear models

Let's start by trying to predict the default using a linear model whose input variable is `balance`, that is 

$$ y = \alpha_0 + \alpha_1 \times balance$$
In homework 3 we learned that using linear models in this setting creates a number of issues for classification, among them the fact that we are not guaranteed that the prediction will correspond to a probability (a number between `0` and `1`)

One way to overcome this is to use the log odds on our response variable. The log odds $y$ of an event with probability $p$ is defined as

$$\tilde y :=\log \left(\frac{p}{1-p}\right)$$

Hence, if $\tilde y$ represents the log odds, we can invert this expression to get the probability.

$$p=\frac{e^{\tilde y}}{1+e^{\tilde y}}=\frac{1}{1+e^{-\tilde y}}$$

# Logistic regression using `tidymodels`

Fortunately for us, logistic models are readily available in R. We can create such a model on the training dataset using the following code:

```{r echo=TRUE}
logit.model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

default.recipe <- 
  recipe(default ~ balance, data=default.train.tbl)

logit.wflow <- workflow() %>%
  add_recipe(default.recipe) %>%
  add_model(logit.model) 

logit.fit <- fit(logit.wflow, default.train.tbl)
```

Finally we can predict the probability of defaulting on the testing dataset or simply predict whether someone would go on default or not

```{r echo=TRUE}
predict(logit.fit, default.test.tbl, type="prob")
predict(logit.fit, default.test.tbl)
```

4. Plot the predicted probability of defaulting (using the logit model) as a function of `balance`.

```{r}
augment(logit.fit, default.test.tbl) %>%
  ggplot(aes(x = balance, y = .pred_Yes)) +
    geom_line()
```

5. How many observations are predicted to default in your testing dataset? How many of your predictions are wrong?
```{r}
default.test.tbl.2 <- augment(logit.fit, default.test.tbl)

table(default.test.tbl.2$.pred_class)

mean(default.test.tbl.2$.pred_class != default.test.tbl.2$default)
```
  No  Yes 
1970   30    

FALSE  TRUE 
 1948    52 
 
2.6% of predictions were wrong.
Note that if there is a classifier trying to predict "No", then the maximum value for wrong predictions is 3.3% in this data because 30/1970 = 3.3%. 
This data is skewed.
asz

 
6. `yardstick` allows you to evaluate the performance of your classification model in many different ways. Consult https://www.tmwr.org/performance.html#binary-classification-metrics and do the following:

a. Calculate the confusion matrix of your model. Is your model making more errors on people that go on default or not?
```{r}
yardstick::conf_mat(data = default.test.tbl.2, truth = default, estimate = .pred_class)
```
          Truth
Prediction   No  Yes
       No  1923   47
       Yes    5   25
diagonal of no-no and yes-yes is good diagonal.

b. Define `accuracy` and calculate it for your model.

```{r}
yardstick::accuracy(data = default.test.tbl.2, truth = default, estimate = .pred_class)
```
.974
percent of correct predictions.

c. Define `specificity`, `sensitivity`, `ROC`, and `AUC`. Plot the ROC curve and calculate the AUC of your model.
```{r}
#specificity and sensitivity
yardstick::roc_curve(data = default.test.tbl.2, truth = default, estimate = .pred_Yes) %>%
  ggplot(aes(x = ))
```
classifier does 4 things:
True positives 
False positives 
False negatives
False positives

specificity: True negatives / (true negatives + false positives)
false positive rate

sensitivity: True positives / (True positives + false negatives)
true positive rate

diagonal = useless test. perfect = 90 degree elbow.

AUR = area under ROC curve. overall accuracy. percentage = percentage accuracy. 50% is useless. coin flip.



 .threshold specificity sensitivity
          <dbl>       <dbl>       <dbl>
 1 -Inf                   0       1    
 2    0.0000273           0       1    
 3    0.0000273           0       0.946
 4    0.0000276           0       0.946
 5    0.0000280           0       0.945
 6    0.0000280           0       0.945
 7    0.0000288           0       0.944
 8    0.0000289           0       0.943
 9    0.0000297           0       0.943
10    0.0000302           0       0.942
...

```{r}
yardstick::roc_auc(data = default.test.tbl.2, truth = default, estimate = .pred_Yes)
```
  .metric .estimator .estimate
  <chr>   <chr>          <dbl>
1 roc_auc binary        0.0487


NOTE HOW TO PLOT IN SOLUTIONS.
ROC curve is .951. (roc_auc) 
******
Crap I missed what the plot means. 

7. (*Optional*) Calculate the AUC and plot the ROC for the logistic regression model that takes into account `balance`, `income` and `student`.

8. (*Optional*) Plot the decision boundary for the logistic regression model with inputs of balance, income and student.



