---
title: "Classification-2"
author: "Jaime Davila"
date: "2/23/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
library(dslabs)
```

generally: training should be bigger than testing. 5:1 or 10:1 ratio of data.


# Is it a 2 or 7? - continuation

Last time we started considering the problem of labeling digits as `2` or a `7` using the following variables (features):

* `x_1` will be the proportion of dark pixels in the upper left quadrant.
* `x_2` will be the proportion of dark pixels in the lower right quadrant.

Let's start by loading the dataset `mnist_27` from `dslabs` and creating our testing and training datasets:

```{r echo=TRUE}
data("mnist_27")
mnist.train.tbl <- tibble(mnist_27$train)
mnist.test.tbl <- tibble(mnist_27$test)

mnist_27
```

And let's note the dimensions of those datasets

```{r echo=TRUE}
dim(mnist.train.tbl)
dim(mnist.test.tbl)
```

Today we are interested in defining the *decision boundary* of the best theoretical classifier which we will call the *Bayes' boundary*. The `mnist` dataset has over 60,000 digits so we can approximate the theoretical probability of a 7 (compared to a 2). Luckily for us this information is contained in the field `true_p` of `mnist_27`. Let's take a look at it:

```{r echo=TRUE}
mnist.true.tbl <-  tibble(mnist_27$true_p)
mnist.true.tbl
```

The way to interpret this table is to note that given `x_1` and `x_2` it provides an estimate of the probability of a digit been a `7`. Let's plot how this probability looks like in two similar ways

```{r echo=TRUE}
ggplot(mnist.true.tbl, aes(x_1, x_2, fill = p)) +
  geom_raster() +
  scale_fill_viridis_c()
```

```{r echo=TRUE}
ggplot(mnist.true.tbl, aes(x_1, x_2, fill = p)) +
  geom_raster() +
  scale_fill_viridis_b()
```

Notice how points on the far right are likely to be `7`, and how points in the left are very likely to be `2`. Finally notice the probability changes around a curved region in the left of the screen.

The Bayes' boundary consists of all the points where the probability is exactly equal to 0.5. We can plot this boundary by using the `stat_contour` command of `ggplot`. Notice that for `stat_contour` to work you need to define `z` in your `aes` command:

```{r echo=FALSE}
ggplot(mnist.true.tbl, aes(x_1, x_2, z=p,fill = p)) +
  geom_raster() +
  stat_contour(breaks=c(0.5), color="pink")+
  scale_fill_viridis_b()
```

In the following exercises we will explore the decision boundary generated by our KNN classifier using the following steps:

1. Using a value of `kNear` of 10, create a KNN model using your training dataset
```{r}
kNear = 10
knn.model <- knn3(y~x_1+x_2, data=mnist.train.tbl, k=kNear)
#  knn3 because discrete and not continuous
knn.model
```

2. We would like to visualize the values of our KNN model across all of the points of the unit square. However our testing dataset does not contain enough of those points so we need to create a tibble with a big amount of points from the unit square interval. We will do that in the following steps

a. Create a vector `grid.vec` that contains the numbers 0, 0.1, ...,1. Make use of the function `seq`.
```{r}
grid.vec <- seq(from = 0, to = 1, by = .1)
grid.vec
```

b. Look at the documentation of the `expand_grid` function from the `tidyverse`, and create a tibble `grid.tbl` with two columns, `x_1` and `x_2` which contains a grid of combinations of two points from 0 to 1 by steps of 0.1
```{r}
grid.tbl <- expand_grid(x_1 = grid.vec, x_2 = grid.vec)
grid.tbl
```

c. Evaluate your KNN model on the values of `grid.tbl`. Create a new column `prob` in `grid.tbl` with the predicted probability of being a 7.
```{r}
pred <- predict(knn.model, grid.tbl) 
pred[,2]
grid.tbl <- grid.tbl %>%
  mutate(prob_7 = pred[,2]) %>%
  select(x_1, x_2, prob_7)
grid.tbl
```

d. Use `grid.tbl` to plot the predicted probability across the unit grid and plot the *decision boundary*.
```{r}
ggplot(grid.tbl, aes(x_1, x_2, z=prob_7,fill = prob_7)) +
  geom_raster() +
  stat_contour(breaks=c(0.5), color="pink")+
  scale_fill_viridis_b()
```


e. It seems your graph is too pixelated. Create a function `plot_knn_model(kNear, grid.dist, train.tbl)` that trains a KNN model with parameter `kNear` on `train.tbl` and displays the value of the probability of being a 7 on a grid of points generated every `grid.dist`. Evaluate your function using `grid.dist` equals to 0.1, 0.03 and 0.01

```{r}
plot_knn_model <- function(kNear, grid.dist, train.tbl) {
  #train a knn model with kNear on train.tbl
  #display the value of the probability of being a 7 on a grid of points generated every grid.dist.
  #evaluate function using grid.dist == 0.1, 0.03, and 0.01
  
  #this is just the previous parts
  knn.model <- knn3(y~x_1+x_2, data=train.tbl, k=kNear) #  knn3 because discrete and not continuous
  grid.vec <- seq(from = 0, to = 1, by = grid.dist)
  grid.tbl <- expand_grid(x_1 = grid.vec, x_2 = grid.vec)
  pred <- predict(knn.model, grid.tbl) 
  grid.tbl <- grid.tbl %>%
    mutate(prob_7 = pred[,2]) %>%
    select(x_1, x_2, prob_7)
  
  ggplot(grid.tbl, aes(x_1, x_2, z=prob_7,fill = prob_7)) +
    geom_raster() +
    stat_contour(breaks=c(0.5), color="pink")+
    scale_fill_viridis_b() +
    ggtitle(str_c("kNear = ", kNear))
}
plot_knn_model(10,0.03, mnist.train.tbl)
plot_knn_model(20,0.01, mnist.train.tbl)

```

3. Experiment plotting with different values of k (say from k=5 to k=50, using steps of 5). Which decision boundary looks more similar to the Bayes boundary? Is this consistent with the optimal value of `k` that you found using in point 4 of our last activity, `3_Classification.Rmd`?
```{r}
plot_knn_model(10,0.01, mnist.train.tbl)
plot_knn_model(20,0.01, mnist.train.tbl)
plot_knn_model(30,0.01, mnist.train.tbl)
plot_knn_model(40,0.01, mnist.train.tbl)
plot_knn_model(50,0.01, mnist.train.tbl)
```
I think it looks better closer to 50 which is above ~35. (prof says about 40 from prev class)
prof says between 40-50 is best I think

# Decision boundary of a linear classifer

4. We can also use a linear model to approximate the probability of being a `7`. Notice that in order to make this approach work, we need to create a new input variable where `2`s are encoded as zeros and `7`s are encoded as ones. Also note that the linear model can give values outside of $[0,1]$ so we will need to truncate predictions that are negative to 0 and prediction over 1 to 1. Implement this approach and plot the boundary of this classifier. How does this boundary compare to the boundary generated by the KNN model?

```{r}

```


# The Default dataset

In the following sets of exercises we will be exploring the `Default` dataset available from the `ISLR2` package. In particular we will construct linear models that would allow us to predict whether a particular person would go on default. 

```{r}
library(ISLR2)
data(Default)
default.tbl <- tibble(Default)
```


5. 

    a. Generate a plot of balance (x) and income (y) vs default (using color). What trends do you observe?  

    b. Divide the original datasets into a training (8000 elements) and a testing dataset (2000 elements) by selecting at random using `slice_sample` and `set_diff` from `tidyverse`. Please keep the `set.seed` command so that your result is reproducible. 

```{r}
set.seed(12345)
```


6. Create a linear model (similar to point 4) that predicts `default` based on `balance` and `income`. What is the missclassification rate?

7. Plot the probability of the model created on 6) on a grid where $(x_1,x_2) \in [0,3000]\times[0,80000]$. Make sure your grid **does not have over 10,000 points**. Plot the decision boundary of the model as well. 

8. Does `default` change depending on whether somebody is a `student` or not? Illustrate your answer using a plot using facets.

9. Create a linear model that uses `student`, `balance`, and `income` to predict `default`. What is the missclassification rate of this model? Are the results better than the model created in 6?

10. Plot the probability and the decision boundary for the model created in 9.




project:
go into mnist dataset
pull out two numbers (like 1 and 4)
make training and testing dataset
create 2 features that allow to distinguish between a 1 and a 4
for example: measure symmetry. 1 is symmetrical. 4 is not symmetrical. 
important thing: calculate those features from the original dataset. 

